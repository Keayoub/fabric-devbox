{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a4f8ed6",
   "metadata": {},
   "source": [
    "# Accessing Fabric OneLake from Azure ML Notebook\n",
    "\n",
    "This notebook is designed to run **inside Azure ML compute instances/clusters**.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Running on Azure ML Compute Instance or Compute Cluster\n",
    "- OneLake datastore already registered\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6381153",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbee94a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install/upgrade required packages\n",
    "#%pip install --upgrade azure-ai-ml azure-identity pandas pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd8f8ad",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set your workspace details and datastore name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ff719f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure ML Workspace Configuration\n",
    "SUBSCRIPTION_ID = \"<your-subscription-id>\"\n",
    "RESOURCE_GROUP = \"<your-resource-group>\"\n",
    "WORKSPACE_NAME = \"<your-workspace-name>\"\n",
    "\n",
    "# OneLake Datastore Name (registered via register-with-cli.ps1)\n",
    "DATASTORE_NAME = \"onelakesp_datastore\" \n",
    "\n",
    "print(f\"Workspace: {WORKSPACE_NAME}\")\n",
    "print(f\"Datastore: {DATASTORE_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f203dc",
   "metadata": {},
   "source": [
    "## 3. Connect Using Managed Identity (Azure ML Compute)\n",
    "\n",
    "On Azure ML compute, we use the **compute's managed identity** - no browser needed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173c1226",
   "metadata": {},
   "source": [
    "### Authentication Options\n",
    "\n",
    "This notebook supports multiple authentication methods:\n",
    "\n",
    "1. **Managed Identity** - Works automatically on Azure ML compute\n",
    "2. **Azure CLI** - If you've run `az login` locally\n",
    "3. **Environment Variables** - Service principal via env vars\n",
    "\n",
    "**If running locally**, choose one of these options:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989b0ce6",
   "metadata": {},
   "source": [
    "#### Option A: Install Azure CLI and Login (Recommended for Local Development)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5a540b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Use Azure CLI authentication\n",
    "# 1. Install Azure CLI: https://docs.microsoft.com/en-us/cli/azure/install-azure-cli\n",
    "# 2. Run in terminal: az login\n",
    "# 3. Then run the cells below\n",
    "\n",
    "# Uncomment to check if Azure CLI is installed:\n",
    "# !az --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cac2d38",
   "metadata": {},
   "source": [
    "#### Option B: Use Service Principal via Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55a234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Use Service Principal from your .env file\n",
    "# Set these environment variables (from your azml_onelakesp_datastore.yml):\n",
    "\n",
    "import os\n",
    "\n",
    "# Uncomment and set these values from your service principal:\n",
    "os.environ[\"AZURE_TENANT_ID\"] = \"<your-tenant-id>\"\n",
    "os.environ[\"AZURE_CLIENT_ID\"] = \"<your-client-id>\"\n",
    "os.environ[\"AZURE_CLIENT_SECRET\"] = \"<your-client-secret>\"  # Get from Key Vault or secure store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ec5eb2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Create the Credential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8521d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Credential Chain\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import (\n",
    "    ManagedIdentityCredential,\n",
    "    AzureCliCredential,\n",
    "    ChainedTokenCredential,\n",
    "    EnvironmentCredential\n",
    ")\n",
    "import os\n",
    "\n",
    "# Use credential chain with multiple fallbacks\n",
    "# Priority: Managed Identity -> Azure CLI -> Environment Variables\n",
    "try:\n",
    "    print(\"Trying authentication methods...\\n\")\n",
    "    \n",
    "    credential = ChainedTokenCredential(\n",
    "        ManagedIdentityCredential(),  # Works on Azure ML compute\n",
    "        AzureCliCredential(),  # Works if 'az login' has been run\n",
    "        EnvironmentCredential()  # Uses env vars (AZURE_CLIENT_ID, AZURE_CLIENT_SECRET, AZURE_TENANT_ID)\n",
    "    )\n",
    "    \n",
    "    print(\"Credential chain created\")\n",
    "    print(\"Will try: Managed Identity -> Azure CLI -> Environment Vars\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating credential: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9937c1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Azure ML workspace\n",
    "try:\n",
    "    ml_client = MLClient(\n",
    "        credential=credential,\n",
    "        subscription_id=SUBSCRIPTION_ID,\n",
    "        resource_group_name=RESOURCE_GROUP,\n",
    "        workspace_name=WORKSPACE_NAME\n",
    "    )\n",
    "    \n",
    "    # Test connection\n",
    "    workspace = ml_client.workspaces.get(WORKSPACE_NAME)\n",
    "    print(f\"Connected to workspace: {workspace.name}\")\n",
    "    print(f\"Location: {workspace.location}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {str(e)}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Make sure WORKSPACE_NAME is correct\")\n",
    "    print(\"2. Verify compute has permissions to access the workspace\")\n",
    "    print(\"3. Check subscription ID and resource group\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6e91ec",
   "metadata": {},
   "source": [
    "## 4. List All Available Datastores\n",
    "See what datastores are registered in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c3727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all datastores\n",
    "try:\n",
    "    print(\"Available Datastores:\\n\")\n",
    "    \n",
    "    datastores = ml_client.datastores.list()\n",
    "    \n",
    "    for ds in datastores:\n",
    "        print(f\"  - {ds.name} ({ds.type})\")\n",
    "    \n",
    "    print(f\"\\nLooking for: '{DATASTORE_NAME}'\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not list datastores: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4da7fb6",
   "metadata": {},
   "source": [
    "## 5. Verify OneLake Datastore Registration\n",
    "Check if the datastore was registered correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57502ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the OneLake datastore\n",
    "try:\n",
    "    datastore = ml_client.datastores.get(DATASTORE_NAME)\n",
    "    \n",
    "    print(\"OneLake Datastore Found!\\n\")\n",
    "    print(f\"Name: {datastore.name}\")\n",
    "    print(f\"Type: {datastore.type}\")\n",
    "    \n",
    "    if hasattr(datastore, 'description'):\n",
    "        print(f\"Description: {datastore.description}\")\n",
    "    \n",
    "    print(\"\\nDatastore is ready to use!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Datastore '{DATASTORE_NAME}' not found!\")\n",
    "    print(f\"\\nError: {str(e)}\")\n",
    "    print(\"\\nSOLUTION: Register the datastore first:\")\n",
    "    print(f\"   .\\\\register-with-cli.ps1 -s '{SUBSCRIPTION_ID}' -g '{RESOURCE_GROUP}' -w '{WORKSPACE_NAME}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5202a0df",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Access OneLake Data Using Datastore URI\n",
    "\n",
    "Once the datastore is verified, you can access files directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559ceab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Construct FULL datastore URI (required for Azure ML Compute Clusters)\n",
    "# Format: azureml://subscriptions/{sub}/resourcegroups/{rg}/workspaces/{ws}/datastores/{datastore}/paths/{path}\n",
    "\n",
    "# Example: Read a CSV file\n",
    "# NOTE: OneLake datastore paths do NOT include \"Files/\" prefix\n",
    "# Use just the folder/filename relative to the Files folder\n",
    "file_path = \"RawData/your-file.csv\"  # Replace with your file path (no \"Files/\" prefix)\n",
    "\n",
    "# FULL URI format (required for Compute Clusters)\n",
    "datastore_uri = f\"azureml://subscriptions/{SUBSCRIPTION_ID}/resourcegroups/{RESOURCE_GROUP}/workspaces/{WORKSPACE_NAME}/datastores/{DATASTORE_NAME}/paths/{file_path}\"\n",
    "\n",
    "print(f\"Datastore URI:\\n{datastore_uri}\")\n",
    "print(f\"\\nPath format: Use folder/file.csv (without 'Files/' prefix)\")\n",
    "print(\"Using FULL URI format (required for Compute Clusters)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cc3b5a",
   "metadata": {},
   "source": [
    "### Read CSV from OneLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8993bac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file from OneLake with automatic encoding detection\n",
    "try:\n",
    "    # Try UTF-8 first (default)\n",
    "    df = pd.read_csv(datastore_uri)\n",
    "    print(f\"Data loaded successfully!\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(f\"\\nFirst 5 rows:\")\n",
    "    display(df.head())\n",
    "\n",
    "except UnicodeDecodeError as e:\n",
    "    # UTF-8 failed, try alternative encodings\n",
    "    print(f\"UTF-8 encoding failed, trying alternative encodings...\")\n",
    "    encodings_to_try = [\"latin1\", \"iso-8859-1\", \"cp1252\", \"utf-16\"]\n",
    "    for encoding in encodings_to_try:\n",
    "        try:\n",
    "            print(f\"Trying {encoding}...\")\n",
    "            df = pd.read_csv(datastore_uri, encoding=encoding)\n",
    "            print(f\"Data loaded successfully with {encoding} encoding!\")\n",
    "            print(f\"Shape: {df.shape}\")\n",
    "            print(f\"Columns: {list(df.columns)}\")\n",
    "            print(f\"\\nFirst 5 rows:\")\n",
    "            display(df.head())\n",
    "            break\n",
    "        except Exception as enc_error:\n",
    "            print(f\"{encoding} failed\")\n",
    "            continue\n",
    "    else:\n",
    "        print(f\"\\nCould not decode file with any common encoding\")\n",
    "        print(f\"Try specifying encoding manually:\")\n",
    "        print(f\"   df = pd.read_csv(datastore_uri, encoding='latin1')\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "    print(\"\\nCheck the file path in your OneLake lakehouse\")\n",
    "    print(\"Remember: Don't include 'Files/' prefix in the path\")\n",
    "    print(f\"Current path: '{file_path}'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error reading file: {str(e)}\")\n",
    "    print(f\"\\nMake sure:\")\n",
    "    print(f\"1. Datastore '{DATASTORE_NAME}' is registered\")\n",
    "    print(f\"2. File path is correct (no 'Files/' prefix): '{file_path}'\")\n",
    "    print(f\"3. Service principal has read access to the lakehouse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7187548",
   "metadata": {},
   "source": [
    "### Handle Different File Encodings\n",
    "\n",
    "If you know your file's encoding, specify it directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dbfaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Read file with specific encoding\n",
    "file_with_special_chars = \"RawData/YourFile.csv\"\n",
    "\n",
    "# FULL URI format (required for Compute Clusters)\n",
    "file_uri = f\"azureml://subscriptions/{SUBSCRIPTION_ID}/resourcegroups/{RESOURCE_GROUP}/workspaces/{WORKSPACE_NAME}/datastores/{DATASTORE_NAME}/paths/{file_with_special_chars}\"\n",
    "\n",
    "# Common encodings for different regions:\n",
    "# - 'latin1' or 'iso-8859-1': Western European (French, Spanish, German)\n",
    "# - 'cp1252': Windows Western European\n",
    "# - 'utf-8': Modern standard (international)\n",
    "# - 'utf-16': Some Windows exports\n",
    "\n",
    "try:\n",
    "    # Option 1: Specify encoding directly\n",
    "    df = pd.read_csv(file_uri, encoding='latin1')  # Most common for Windows files with special chars\n",
    "    \n",
    "    print(f\"File loaded with latin1 encoding\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    display(df.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    print(\"\\nTry these encodings:\")\n",
    "    print(\"   - encoding='latin1' (Western European)\")\n",
    "    print(\"   - encoding='cp1252' (Windows)\")\n",
    "    print(\"   - encoding='utf-16' (Some exports)\")\n",
    "    print(\"   - encoding='iso-8859-1' (Latin-1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e92ffc",
   "metadata": {},
   "source": [
    "### Read Parquet from OneLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774fd4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Read Parquet file\n",
    "parquet_path = \"your-folder/your-data.parquet\"  # Replace with your file path (no \"Files/\" prefix)\n",
    "\n",
    "# FULL URI format (required for Compute Clusters)\n",
    "parquet_uri = f\"azureml://subscriptions/{SUBSCRIPTION_ID}/resourcegroups/{RESOURCE_GROUP}/workspaces/{WORKSPACE_NAME}/datastores/{DATASTORE_NAME}/paths/{parquet_path}\"\n",
    "\n",
    "try:\n",
    "    df_parquet = pd.read_parquet(parquet_uri)\n",
    "    print(f\"Parquet file loaded successfully!\")\n",
    "    print(f\"Shape: {df_parquet.shape}\")\n",
    "    display(df_parquet.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not read parquet file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73ba2c2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Write Data to OneLake\n",
    "\n",
    "Save processed data back to your OneLake lakehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57d11ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "sample_data = pd.DataFrame({\n",
    "    'id': range(1, 11),\n",
    "    'value': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'],\n",
    "    'score': [85.5, 90.2, 78.9, 92.1, 88.7, 95.3, 82.4, 91.6, 87.8, 93.2]\n",
    "})\n",
    "\n",
    "print(\"Sample data created:\")\n",
    "display(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23973571",
   "metadata": {},
   "source": [
    "### Option A: Write Using DataLakeServiceClient (Recommended)\n",
    "\n",
    "This approach uploads bytes directly to OneLake using the Azure Data Lake SDK. Works with Managed Identity on Azure ML compute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62b9814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Upload using DataLakeServiceClient (recommended)\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "\n",
    "# Configure these for your OneLake environment\n",
    "# Get these from your Fabric workspace\n",
    "ONELAKE_WORKSPACE_ID = \"<your-fabric-workspace-id>\"  # From Fabric workspace URL\n",
    "LAKEHOUSE_ID = \"<your-lakehouse-id>\"  # From lakehouse settings in Fabric\n",
    "\n",
    "# Define output path (relative to Files/ folder)\n",
    "output_file_path = \"Processed/processed_data.csv\"\n",
    "\n",
    "print(f\"Uploading data to OneLake...\")\n",
    "print(f\"Target: Files/{output_file_path}\\n\")\n",
    "\n",
    "try:\n",
    "    # Create DataLake client for OneLake\n",
    "    onelake_endpoint = \"https://onelake.dfs.fabric.microsoft.com\"\n",
    "    \n",
    "    service_client = DataLakeServiceClient(\n",
    "        account_url=onelake_endpoint,\n",
    "        credential=credential  # Uses the credential from authentication section above\n",
    "    )\n",
    "    \n",
    "    print(f\"Step 1: Connecting to OneLake\")\n",
    "    print(f\"  Workspace: {ONELAKE_WORKSPACE_ID}\")\n",
    "    print(f\"  Lakehouse: {LAKEHOUSE_ID}\")\n",
    "    \n",
    "    # Get file client\n",
    "    # OneLake path format: {workspace}/{lakehouse}/Files/{path}\n",
    "    file_system_client = service_client.get_file_system_client(file_system=ONELAKE_WORKSPACE_ID)\n",
    "    full_path = f\"{LAKEHOUSE_ID}/Files/{output_file_path}\"\n",
    "    file_client = file_system_client.get_file_client(full_path)\n",
    "    \n",
    "    # Convert DataFrame to CSV bytes\n",
    "    csv_bytes = sample_data.to_csv(index=False).encode(\"utf-8\")\n",
    "    \n",
    "    print(f\"\\nStep 2: Uploading CSV ({len(csv_bytes)} bytes)\")\n",
    "    \n",
    "    # Upload (overwrite if exists)\n",
    "    file_client.upload_data(csv_bytes, overwrite=True)\n",
    "    \n",
    "    print(f\"\\n✓ SUCCESS: Data uploaded to OneLake!\")\n",
    "    print(f\"  Total records: {len(sample_data)}\")\n",
    "    print(f\"  Location: {ONELAKE_WORKSPACE_ID}/{LAKEHOUSE_ID}/Files/{output_file_path}\")\n",
    "    \n",
    "    print(f\"\\nYou can access this file from:\")\n",
    "    print(f\"  - Microsoft Fabric Lakehouse UI (Files/{output_file_path})\")\n",
    "    print(f\"  - Other Azure ML notebooks\")\n",
    "    print(f\"  - Azure ML jobs/pipelines\")\n",
    "    \n",
    "    # Optional: Verify upload by downloading a preview\n",
    "    print(f\"\\nStep 3: Verifying upload...\")\n",
    "    downloaded = file_client.download_file().readall()\n",
    "    print(f\"✓ Verified - downloaded {len(downloaded)} bytes\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Upload failed: {str(e)}\")\n",
    "    \n",
    "    # Check for common configuration issues\n",
    "    if \"<your-\" in ONELAKE_WORKSPACE_ID or \"<your-\" in LAKEHOUSE_ID:\n",
    "        print(\"\\nConfiguration needed:\")\n",
    "        print(\"  1. Set ONELAKE_WORKSPACE_ID (find in Fabric workspace URL)\")\n",
    "        print(\"  2. Set LAKEHOUSE_ID (find in lakehouse settings)\")\n",
    "    \n",
    "    print(\"\\nAdditional troubleshooting:\")\n",
    "    print(\"  - Ensure compute identity has WRITE permissions to the lakehouse\")\n",
    "    print(\"  - Check if OneLake Access Protection (OAP) is enabled\")\n",
    "    print(\"  - Verify the lakehouse exists and is accessible\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5840594",
   "metadata": {},
   "source": [
    "### Option B: Write Using Azure ML Datastore Upload (Alternative)\n",
    "\n",
    "This approach saves to a temp file locally, then uploads via Azure ML Datastore API. Requires `azureml-core` package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de72a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Save locally then upload via Azure ML Datastore\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Ensure azureml-core is installed\n",
    "try:\n",
    "    from azureml.core import Workspace, Datastore\n",
    "except ImportError:\n",
    "    print(\"Installing azureml-core...\")\n",
    "    %pip install azureml-core\n",
    "    from azureml.core import Workspace, Datastore\n",
    "\n",
    "output_folder = \"output\"  # Folder inside datastore (no \"Files/\" prefix)\n",
    "\n",
    "try:\n",
    "    # Save DataFrame to a temp file\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".csv\", delete=False, mode='w') as tmp:\n",
    "        local_path = tmp.name\n",
    "        sample_data.to_csv(local_path, index=False)\n",
    "    \n",
    "    print(f\"Saved temp file: {local_path}\")\n",
    "    \n",
    "    # Get workspace using the existing ml_client credential\n",
    "    ws = Workspace(\n",
    "        subscription_id=SUBSCRIPTION_ID,\n",
    "        resource_group=RESOURCE_GROUP,\n",
    "        workspace_name=WORKSPACE_NAME,\n",
    "        auth=credential\n",
    "    )\n",
    "    \n",
    "    # Get the datastore\n",
    "    ds = Datastore.get(ws, DATASTORE_NAME)\n",
    "    \n",
    "    # Upload to the datastore\n",
    "    # target_path is relative to the datastore mapping (no \"Files/\" prefix)\n",
    "    ds.upload_files(\n",
    "        files=[local_path],\n",
    "        target_path=output_folder,\n",
    "        overwrite=True,\n",
    "        show_progress=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nSUCCESS: Data uploaded via Datastore.upload_files!\")\n",
    "    print(f\"Location: {output_folder}/\")    \n",
    "    # Clean up temp file\n",
    "    os.remove(local_path)\n",
    "    print(f\"\\nSUCCESS: Temp file cleaned up\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Upload failed: {str(e)}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Ensure azureml-core is installed\")\n",
    "    print(\"2. Verify datastore name is correct\")\n",
    "    print(\"3. Check that the compute identity has write permissions\")\n",
    "    print(\"4. Make sure Workspace.from_config() can authenticate\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3984e9fc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Common File Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dcf4d7",
   "metadata": {},
   "source": [
    "### Pattern 1: Load Multiple CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9535956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multiple files from a folder\n",
    "folder_path = \"data\"  # Your folder in OneLake (no \"Files/\" prefix)\n",
    "file_names = [\"file1.csv\", \"file2.csv\", \"file3.csv\"]  # List your files\n",
    "\n",
    "dfs = []\n",
    "for file_name in file_names:\n",
    "    try:\n",
    "        # FULL URI format (required for Compute Clusters)\n",
    "        file_uri = f\"azureml://subscriptions/{SUBSCRIPTION_ID}/resourcegroups/{RESOURCE_GROUP}/workspaces/{WORKSPACE_NAME}/datastores/{DATASTORE_NAME}/paths/{folder_path}/{file_name}\"\n",
    "        df_temp = pd.read_csv(file_uri)\n",
    "        dfs.append(df_temp)\n",
    "        print(f\"Loaded: {file_name} ({df_temp.shape[0]} rows)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Skipped {file_name}: {str(e)}\")\n",
    "\n",
    "if dfs:\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"\\nCombined {len(dfs)} files\")\n",
    "    print(f\"Total rows: {combined_df.shape[0]}\")\n",
    "else:\n",
    "    print(\"\\nNo files loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bba20f7",
   "metadata": {},
   "source": [
    "### Pattern 2: Process Large Files in Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dafecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read large file in chunks\n",
    "folder_path = \"data\"  # Your folder in OneLake (no \"Files/\" prefix)\n",
    "large_file_path = \"large_file.csv\"  # Your file path (no \"Files/\" prefix)\n",
    "\n",
    "# FULL URI format (required for Compute Clusters)\n",
    "large_file_uri = f\"azureml://subscriptions/{SUBSCRIPTION_ID}/resourcegroups/{RESOURCE_GROUP}/workspaces/{WORKSPACE_NAME}/datastores/{DATASTORE_NAME}/paths/{folder_path}/{large_file_path}\"\n",
    "\n",
    "try:\n",
    "    chunk_size = 10000  # Rows per chunk\n",
    "    chunks_processed = 0\n",
    "    \n",
    "    for chunk in pd.read_csv(large_file_uri, chunksize=chunk_size):\n",
    "        # Process each chunk\n",
    "        chunks_processed += 1\n",
    "        print(f\"Processing chunk {chunks_processed}: {len(chunk)} rows\")\n",
    "        \n",
    "        # Your processing logic here\n",
    "        # ...\n",
    "    \n",
    "    print(f\"\\nProcessed {chunks_processed} chunks\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not process file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d67e9f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Troubleshooting & Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025dd54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_connection():\n",
    "    \"\"\"Run diagnostics to troubleshoot connection issues\"\"\"\n",
    "    print(\"Running Diagnostics...\\n\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Check workspace connection\n",
    "    print(\"\\n1. Workspace Connection\")\n",
    "    try:\n",
    "        ws = ml_client.workspaces.get(WORKSPACE_NAME)\n",
    "        print(f\"   Connected to: {ws.name}\")\n",
    "        print(f\"   Location: {ws.location}\")\n",
    "        print(f\"   Resource Group: {ws.resource_group}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Connection failed: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    # 2. Check datastore\n",
    "    print(\"\\n2. Datastore Verification\")\n",
    "    try:\n",
    "        ds = ml_client.datastores.get(DATASTORE_NAME)\n",
    "        print(f\"   Datastore found: {ds.name}\")\n",
    "        print(f\"   Type: {ds.type}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Datastore not found: {str(e)}\")\n",
    "        print(\"\\n   To register the datastore, run:\")\n",
    "        print(f\"      .\\\\register-with-cli.ps1 -s '{SUBSCRIPTION_ID}' -g '{RESOURCE_GROUP}' -w '{WORKSPACE_NAME}'\")\n",
    "        return\n",
    "    \n",
    "    # 3. Test URI construction\n",
    "    print(\"\\n3. URI Format Test\")\n",
    "    test_uri = f\"azureml://subscriptions/{SUBSCRIPTION_ID}/resourcegroups/{RESOURCE_GROUP}/workspaces/{WORKSPACE_NAME}/datastores/{DATASTORE_NAME}/paths/test.csv\"\n",
    "    print(f\"   FULL URI format (required for Compute Clusters):\")\n",
    "    print(f\"      {test_uri}\")\n",
    "    \n",
    "    # 4. Environment check\n",
    "    print(\"\\n4. Environment Check\")\n",
    "    import sys\n",
    "    print(f\"   Python version: {sys.version.split()[0]}\")\n",
    "    print(f\"   Pandas version: {pd.__version__}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Diagnostics Complete!\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"   1. Update 'file_path' with your actual OneLake file\")\n",
    "    print(\"   2. Run the data access cells above\")\n",
    "    print(\"   3. If issues persist, check service principal permissions\")\n",
    "\n",
    "# Run diagnostics\n",
    "diagnose_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48b1f75",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Quick Reference\n",
    "\n",
    "### Your Configuration\n",
    "```python\n",
    "SUBSCRIPTION_ID = \"<your-subscription-id>\"\n",
    "RESOURCE_GROUP = \"<your-resource-group>\"\n",
    "WORKSPACE_NAME = \"<your-workspace-name>\"\n",
    "DATASTORE_NAME = \"onelakesp_datastore\"\n",
    "```\n",
    "\n",
    "### Authentication Methods\n",
    "1. **Managed Identity** - Automatic on Azure ML compute (recommended)\n",
    "2. **Azure CLI** - Run `az login` locally\n",
    "3. **Environment Variables** - Set AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET\n",
    "\n",
    "### URI Format (FULL - Required for Compute Clusters)\n",
    "```\n",
    "azureml://subscriptions/{subscription}/resourcegroups/{resource_group}/workspaces/{workspace}/datastores/{datastore_name}/paths/{path}\n",
    "```\n",
    "\n",
    "**IMPORTANT:** Azure ML Compute Clusters require the FULL URI format with subscription, resource group, and workspace details.\n",
    "\n",
    "### Path Format Rule\n",
    "- In Fabric Portal: Files are shown as \"Files/RawData/file.csv\"\n",
    "- In Azure ML Datastore: Use \"RawData/file.csv\" (omit \"Files/\" prefix)\n",
    "- Reason: Datastore automatically references the Files/ folder\n",
    "\n",
    "### Common Operations\n",
    "```python\n",
    "# FULL URI format\n",
    "base_uri = f\"azureml://subscriptions/{SUBSCRIPTION_ID}/resourcegroups/{RESOURCE_GROUP}/workspaces/{WORKSPACE_NAME}/datastores/{DATASTORE_NAME}/paths\"\n",
    "\n",
    "# Read CSV\n",
    "df = pd.read_csv(f\"{base_uri}/data.csv\")\n",
    "\n",
    "# Read CSV with encoding\n",
    "df = pd.read_csv(f\"{base_uri}/data.csv\", encoding='latin1')\n",
    "\n",
    "# Read Parquet\n",
    "df = pd.read_parquet(f\"{base_uri}/data.parquet\")\n",
    "\n",
    "# Write CSV\n",
    "df.to_csv(f\"{base_uri}/output.csv\", index=False)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
