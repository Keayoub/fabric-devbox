{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3fa4b47",
   "metadata": {},
   "source": [
    "# Access OneLake Files Directly (Without Azure ML)\n",
    "\n",
    "This notebook accesses OneLake files **directly** using Azure Storage SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f39ddd",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9791732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Azure Storage SDK for Data Lake\n",
    "%pip install azure-storage-file-datalake azure-identity pandas pyarrow python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aca1c7",
   "metadata": {},
   "source": [
    "## 2. Load Credentials from .env File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc54be1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file\n",
    "env_path = r\"your-environment-file-path\"\n",
    "load_dotenv(env_path)\n",
    "\n",
    "# Get credentials\n",
    "TENANT_ID = os.getenv(\"FABRIC_TENANT_ID\")\n",
    "CLIENT_ID = os.getenv(\"FABRIC_APP_ID\")\n",
    "CLIENT_SECRET = os.getenv(\"FABRIC_APP_SECRET\")\n",
    "WORKSPACE_ID = os.getenv(\"FABRIC_WORKSPACE_ID\")\n",
    "\n",
    "print(f\"‚úÖ Loaded credentials\")\n",
    "print(f\"   Tenant: {TENANT_ID}\")\n",
    "print(f\"   Client: {CLIENT_ID}\")\n",
    "print(f\"   Workspace: {WORKSPACE_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4168e8b5",
   "metadata": {},
   "source": [
    "## 3. Configure OneLake Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db8991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneLake Configuration\n",
    "ONELAKE_ENDPOINT = \"https://msit-onelake.dfs.fabric.microsoft.com\"\n",
    "LAKEHOUSE_ID = \"b5607519-ec4b-4a83-ac2a-5443c8887e2a\"\n",
    "\n",
    "# Your file path in OneLake\n",
    "FILE_PATH = \"Files/RawData/AddressData.csv\"\n",
    "\n",
    "print(f\"OneLake Endpoint: {ONELAKE_ENDPOINT}\")\n",
    "print(f\"Lakehouse ID: {LAKEHOUSE_ID}\")\n",
    "print(f\"File Path: {FILE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05a60aa",
   "metadata": {},
   "source": [
    "## 4. Authenticate with Service Principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bbb29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import ClientSecretCredential\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "\n",
    "# Create credential\n",
    "credential = ClientSecretCredential(\n",
    "    tenant_id=TENANT_ID,\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET\n",
    ")\n",
    "\n",
    "# Create Data Lake client\n",
    "service_client = DataLakeServiceClient(\n",
    "    account_url=ONELAKE_ENDPOINT,\n",
    "    credential=credential\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Created OneLake client\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b44e37",
   "metadata": {},
   "source": [
    "## 5. Read CSV File from OneLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0482257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "# Get file system (workspace)\n",
    "file_system_client = service_client.get_file_system_client(WORKSPACE_ID)\n",
    "\n",
    "# Get file client\n",
    "file_client = file_system_client.get_file_client(f\"{LAKEHOUSE_ID}/{FILE_PATH}\")\n",
    "\n",
    "# Download file\n",
    "download = file_client.download_file()\n",
    "file_content = download.readall()\n",
    "\n",
    "# Read into pandas\n",
    "df = pd.read_csv(BytesIO(file_content))\n",
    "\n",
    "print(f\"‚úÖ File loaded successfully!\")\n",
    "print(f\"   Shape: {df.shape}\")\n",
    "print(f\"   Columns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46a1e40",
   "metadata": {},
   "source": [
    "## 6. Helper Function: Read Any File from OneLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5847f8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_onelake_csv(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read a CSV file from OneLake\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path relative to lakehouse (e.g., \"Files/data.csv\")\n",
    "    \n",
    "    Returns:\n",
    "        pandas DataFrame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get file client\n",
    "        file_client = file_system_client.get_file_client(f\"{LAKEHOUSE_ID}/{file_path}\")\n",
    "\n",
    "        # Download and read\n",
    "        download = file_client.download_file()\n",
    "        file_content = download.readall()\n",
    "\n",
    "        # Parse CSV\n",
    "        df = pd.read_csv(BytesIO(file_content))\n",
    "\n",
    "        print(f\"‚úÖ Loaded: {file_path}\")\n",
    "        print(f\"   Shape: {df.shape}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading {file_path}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Test the function\n",
    "df_address = read_onelake_csv(\"Files/data.csv\")\n",
    "display(df_address.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78011258",
   "metadata": {},
   "source": [
    "## 7. Read Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b096913c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_onelake_parquet(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read a Parquet file from OneLake\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path relative to lakehouse (e.g., \"Files/data.parquet\")\n",
    "    \n",
    "    Returns:\n",
    "        pandas DataFrame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        file_client = file_system_client.get_file_client(f\"{LAKEHOUSE_ID}/{file_path}\")\n",
    "        download = file_client.download_file()\n",
    "        file_content = download.readall()\n",
    "        \n",
    "        df = pd.read_parquet(BytesIO(file_content))\n",
    "        \n",
    "        print(f\"‚úÖ Loaded: {file_path}\")\n",
    "        print(f\"   Shape: {df.shape}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading {file_path}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Example usage (update with your actual file)\n",
    "# df_parquet = read_onelake_parquet(\"Files/data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e749c6a7",
   "metadata": {},
   "source": [
    "## 8. Write Files to OneLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63f8428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_onelake_csv(df: pd.DataFrame, file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Write a DataFrame to OneLake as CSV\n",
    "    \n",
    "    Args:\n",
    "        df: pandas DataFrame\n",
    "        file_path: Path relative to lakehouse (e.g., \"Files/output.csv\")\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert DataFrame to CSV bytes\n",
    "        csv_buffer = BytesIO()\n",
    "        df.to_csv(csv_buffer, index=False)\n",
    "        csv_bytes = csv_buffer.getvalue()\n",
    "        \n",
    "        # Get file client\n",
    "        file_client = file_system_client.get_file_client(f\"{LAKEHOUSE_ID}/{file_path}\")\n",
    "        \n",
    "        # Upload\n",
    "        file_client.upload_data(csv_bytes, overwrite=True)\n",
    "        \n",
    "        print(f\"‚úÖ Wrote: {file_path}\")\n",
    "        print(f\"   Rows: {len(df)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error writing {file_path}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Example: Write sample data\n",
    "sample_df = pd.DataFrame({\n",
    "    'id': [1, 2, 3],\n",
    "    'name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'value': [100, 200, 300]\n",
    "})\n",
    "\n",
    "write_onelake_csv(sample_df, \"Files/output/sample_output.csv\")\n",
    "print(\"\\nüí° File saved to: Files/output/sample_output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3183a5b2",
   "metadata": {},
   "source": [
    "## 9. List Files in OneLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d49194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_onelake_files(folder_path: str = \"Files\") -> list:\n",
    "    \"\"\"\n",
    "    List files in a OneLake folder\n",
    "    \n",
    "    Args:\n",
    "        folder_path: Folder path relative to lakehouse (e.g., \"Files/RawData\")\n",
    "    \n",
    "    Returns:\n",
    "        List of file paths\n",
    "    \"\"\"\n",
    "    try:\n",
    "        directory_client = file_system_client.get_directory_client(f\"{LAKEHOUSE_ID}/{folder_path}\")\n",
    "        paths = directory_client.get_paths(recursive=False)\n",
    "        \n",
    "        files = []\n",
    "        for path in paths:\n",
    "            # Remove lakehouse ID prefix\n",
    "            clean_path = path.name.replace(f\"{LAKEHOUSE_ID}/\", \"\")\n",
    "            files.append(clean_path)\n",
    "            print(f\"  üìÑ {clean_path}\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Found {len(files)} items\")\n",
    "        return files\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error listing files: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# List files in folder\n",
    "print(\"üìÅ Files in Files:\\n\")\n",
    "files = list_onelake_files(\"Files/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881290a1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Quick Reference\n",
    "\n",
    "### Your Configuration\n",
    "```python\n",
    "ONELAKE_ENDPOINT = \"https://msit-onelake.dfs.fabric.microsoft.com\"\n",
    "WORKSPACE_ID = \"fb53fbfb-d8e9-4797-b2f5-ba80bb9a7388\"  # From FABRIC_WORKSPACE_ID\n",
    "LAKEHOUSE_ID = \"b5607519-ec4b-4a83-ac2a-5443c8887e2a\"\n",
    "```\n",
    "\n",
    "### Read Files\n",
    "```python\n",
    "# CSV\n",
    "df = read_onelake_csv(\"Files/RawData/datafile.csv\")\n",
    "\n",
    "# Parquet\n",
    "df = read_onelake_parquet(\"Files/data.parquet\")\n",
    "```\n",
    "\n",
    "### Write Files\n",
    "```python\n",
    "write_onelake_csv(df, \"Files/output/result.csv\")\n",
    "```\n",
    "\n",
    "### List Files\n",
    "```python\n",
    "files = list_onelake_files(\"Files/RawData\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Differences from Azure ML Notebook\n",
    "\n",
    "| Azure ML Notebook | This Notebook (Direct Access) |\n",
    "|-------------------|--------------------------------|\n",
    "| `azureml://datastores/...` | Direct Azure Storage SDK |\n",
    "| Requires Azure ML workspace | No Azure ML needed |\n",
    "| Only works in Azure ML | Works anywhere |\n",
    "| Uses datastore registration | Direct OneLake connection |\n",
    "\n",
    "---\n",
    "\n",
    "**‚úÖ This notebook works in ANY environment - local, Fabric, or cloud!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
