{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e614cd47",
   "metadata": {},
   "source": [
    "# Fabric Pipeline & Dataflow Monitoring\n",
    "\n",
    "This notebook collects **Pipeline Runs** and **Dataflow Runs** from the Fabric REST APIs and sends them to Azure Log Analytics.\n",
    "\n",
    "**üìñ For complete setup instructions, authentication methods, configuration options, and troubleshooting, see [`fabric_pipeline_dataflow_collector_README.md`](fabric_pipeline_dataflow_collector_README.md)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb26aebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === One-time installs per session Or Use Fabric Environment ===\n",
    "%pip install --quiet msal requests azure-identity azure-keyvault-secrets python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1de72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Parameters (mark this as a parameter cell in Fabric) ===\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Workspace and item configuration\n",
    "workspace_id = os.getenv(\"FABRIC_WORKSPACE_ID\")\n",
    "\n",
    "# List only the items you want to collect. Leave empty lists to skip.\n",
    "pipeline_item_ids = [\n",
    "    # Add your pipeline IDs here, e.g.:\n",
    "    \"fffa88df-ed23-45ba-bb2e-803f0089dc39\",\n",
    "    \"92243a88-c144-4749-8eac-e2dd8e7f9b31\",\n",
    "]\n",
    "dataflow_item_ids = [\n",
    "    # Add your dataflow IDs here, e.g.:\n",
    "    \"bc5c92b0-d58a-487b-8692-965e69345792\",\n",
    "    \"065696af-4621-4538-953c-65899053ae24\",\n",
    "]\n",
    "\n",
    "# === CONFIGURATION MODES ===\n",
    "# OPTION 1: Bulk Ingestion (Historical Data Collection)\n",
    "lookback_minutes = 43200  # 30 days for comprehensive bulk load\n",
    "collect_activity_runs = True  # Disabled to avoid API limits\n",
    "\n",
    "# OPTION 2: Incremental Collection (Regular Monitoring)\n",
    "# lookback_minutes = 1200  # 20 hours for regular incremental collection\n",
    "# collect_activity_runs = True  # Enabled for detailed activity monitoring\n",
    "\n",
    "# üéØ OPTION 3: Activity Runs Backfill (After bulk completion)\n",
    "# lookback_minutes = 10080  # 7 days for recent activity runs\n",
    "# collect_activity_runs = True  # Enabled for detailed data\n",
    "\n",
    "# Lookback window options reference:\n",
    "# - 1200 (20 hours) - Regular incremental runs\n",
    "# - 10080 (7 days) - Good for activity runs backfill\n",
    "# - 43200 (30 days) - Full month bulk ingestion\n",
    "# - 131400 (3 months) - Comprehensive historical data\n",
    "\n",
    "# DCR / Logs Ingestion API settings from environment variables\n",
    "dcr_endpoint_host = os.getenv(\"DCR_ENDPOINT_HOST\")\n",
    "dcr_immutable_id = os.getenv(\"DCR_IMMUTABLE_ID\")\n",
    "\n",
    "# Stream names must match your DCR configuration and map to the LA tables we created\n",
    "stream_pipeline = \"Custom-FabricPipelineRun_CL\"             \n",
    "stream_activity = \"Custom-FabricPipelineActivityRun_CL\"     \n",
    "stream_dataflow = \"Custom-FabricDataflowRun_CL\"            \n",
    "\n",
    "# === Authentication Configuration ===\n",
    "# Basic credentials from environment variables\n",
    "tenant_id = os.getenv(\"FABRIC_TENANT_ID\")\n",
    "client_id = os.getenv(\"FABRIC_APP_ID\")\n",
    "client_secret_env = os.getenv(\"FABRIC_APP_SECRET\")\n",
    "\n",
    "# Key Vault configuration (optional)\n",
    "use_key_vault = False  # Set to True to use Key Vault\n",
    "use_managed_identity = False  # Set to True when running on Azure resources (VM, Container App, etc.)\n",
    "key_vault_uri = os.getenv(\"AZURE_KEY_VAULT_URI\", \"https://kaydemokeyvault.vault.azure.net/\")\n",
    "key_vault_secret_name = os.getenv(\"AZURE_KEY_VAULT_SECRET_NAME\", \"FabricServicePrincipal\")\n",
    "\n",
    "# Authentication options:\n",
    "# 1. Environment variables only: use_key_vault=False\n",
    "# 2. Key Vault with managed identity: use_key_vault=True, use_managed_identity=True\n",
    "# 3. Key Vault with client secret: use_key_vault=True, use_managed_identity=False (requires FABRIC_APP_SECRET)\n",
    "\n",
    "# Validation\n",
    "if not all([tenant_id, client_id, dcr_endpoint_host, dcr_immutable_id]):\n",
    "    missing = []\n",
    "    if not tenant_id: missing.append(\"FABRIC_TENANT_ID\")\n",
    "    if not client_id: missing.append(\"FABRIC_APP_ID\")\n",
    "    if not dcr_endpoint_host: missing.append(\"DCR_ENDPOINT_HOST\")\n",
    "    if not dcr_immutable_id: missing.append(\"DCR_IMMUTABLE_ID\")\n",
    "    print(f\"‚ùå Missing required environment variables: {', '.join(missing)}\")\n",
    "    print(\"   Please check your .env file or environment configuration\")\n",
    "else:\n",
    "    print(\"‚úÖ Environment variables loaded successfully\")\n",
    "\n",
    "if not client_secret_env and not use_key_vault:\n",
    "    print(\"‚ö†Ô∏è  Warning: No authentication method configured\")\n",
    "    print(\"   Either set FABRIC_APP_SECRET or configure Key Vault\")\n",
    "\n",
    "print(\"\\n Data Collection Configuration:\")\n",
    "print(f\"  Workspace ID: {workspace_id or 'Not set'}\")\n",
    "print(f\"  Lookback Window: {lookback_minutes:,} minutes ({lookback_minutes/1440:.1f} days)\")\n",
    "print(f\"  Collect Activity Runs: {collect_activity_runs}\")\n",
    "print(f\"  Pipeline Items: {len(pipeline_item_ids)}\")\n",
    "print(f\"  Dataflow Items: {len(dataflow_item_ids)}\")\n",
    "print()\n",
    "print(\"Authentication Configuration:\")\n",
    "print(f\"  Tenant ID: {tenant_id[:8] + '...' if tenant_id else 'Not set'}\")\n",
    "print(f\"  Client ID: {client_id[:8] + '...' if client_id else 'Not set'}\")\n",
    "print(f\"  Use Key Vault: {use_key_vault}\")\n",
    "print(f\"  Use Managed Identity: {use_managed_identity}\")\n",
    "print(f\"  Environment Secret Available: {'Yes' if client_secret_env else 'No'}\")\n",
    "print(f\"  Key Vault URI: {key_vault_uri if use_key_vault else 'Not used'}\")\n",
    "print()\n",
    "print(\"Data Collection Endpoint Configuration:\")\n",
    "print(f\"  DCR Endpoint: {dcr_endpoint_host or 'Not set'}\")\n",
    "print(f\"  DCR Immutable ID: {dcr_immutable_id[:8] + '...' if dcr_immutable_id else 'Not set'}\")\n",
    "print()\n",
    "\n",
    "# Configuration mode detection\n",
    "if lookback_minutes > 10080 and not collect_activity_runs:  # More than 7 days, no activities\n",
    "    print(\"BULK INGESTION MODE\")\n",
    "    print(\"   - Large lookback window for historical data\")\n",
    "    print(\"   - Activity runs disabled to avoid API limits\")\n",
    "    print(\"   - After completion, switch to incremental mode\")\n",
    "elif lookback_minutes <= 2880 and collect_activity_runs:  # 2 days or less with activities\n",
    "    print(\"INCREMENTAL COLLECTION MODE\")\n",
    "    print(\"   - Short lookback window for regular monitoring\")\n",
    "    print(\"   - Activity runs enabled for detailed insights\")\n",
    "    print(\"   - Suitable for scheduled execution\")\n",
    "elif collect_activity_runs:  # Medium window with activities\n",
    "    print(\"ACTIVITY RUNS BACKFILL MODE\")\n",
    "    print(\"   - Medium lookback window for activity collection\")\n",
    "    print(\"   - Collecting detailed activity data\")\n",
    "    print(\"   - Monitor API rate limits\")\n",
    "else:\n",
    "    print(\"CUSTOM CONFIGURATION\")\n",
    "    print(\"   - Review settings based on your requirements\")\n",
    "\n",
    "if lookback_minutes > 10080:  # More than 7 days\n",
    "    print(\"   - Consider running during off-peak hours\")\n",
    "    print(\"   - Monitor API rate limits and Log Analytics ingestion limits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8fe8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Fabric-Aware Authentication Helpers ===\n",
    "def get_fabric_token(scope: str = \"https://api.fabric.microsoft.com/.default\") -> str:\n",
    "    \"\"\"\n",
    "    Get authentication token with Fabric-aware logic:\n",
    "    1. Try Fabric workspace identity (if available)\n",
    "    2. Fall back to service principal authentication\n",
    "    3. Support both local and Fabric environments\n",
    "    \"\"\"\n",
    "    \n",
    "    # First, try Fabric's built-in authentication if available\n",
    "    try:\n",
    "        import notebookutils\n",
    "        print(f\"[Auth] Attempting Fabric workspace identity for {scope}\")\n",
    "        \n",
    "        # Use Fabric's credential system if available\n",
    "        token = notebookutils.credentials.getSecret(\"System\", \"AccessToken\")\n",
    "        if token:\n",
    "            print(f\"[Auth] ‚úÖ Successfully acquired token via Fabric workspace identity\")\n",
    "            return token\n",
    "        else:\n",
    "            print(f\"[Auth] ‚ö†Ô∏è  Fabric workspace token not available, falling back to service principal\")\n",
    "            \n",
    "    except (ImportError, AttributeError, Exception) as e:\n",
    "        print(f\"[Auth] Fabric authentication not available: {str(e)[:100]}\")\n",
    "        print(f\"[Auth] Using service principal authentication\")\n",
    "    \n",
    "    # Fall back to standard service principal authentication\n",
    "    return acquire_token_client_credentials(tenant_id, client_id, client_secret_env, scope)\n",
    "\n",
    "def get_credentials_fabric_aware():\n",
    "    \"\"\"\n",
    "    Get authentication credentials with Fabric runtime awareness\n",
    "    Returns tuple: (tenant_id, client_id, client_secret, use_fabric_auth)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if running in Fabric\n",
    "    try:\n",
    "        import notebookutils\n",
    "        running_in_fabric = True\n",
    "        \n",
    "        # Try to get credentials from Fabric Key Vault integration\n",
    "        try:\n",
    "            fabric_tenant = notebookutils.credentials.getSecret(\"Fabric\", \"TenantId\")\n",
    "            fabric_client_id = notebookutils.credentials.getSecret(\"Fabric\", \"ClientId\") \n",
    "            fabric_secret = notebookutils.credentials.getSecret(\"Fabric\", \"ClientSecret\")\n",
    "            \n",
    "            if fabric_tenant and fabric_client_id:\n",
    "                print(\"[Auth] ‚úÖ Using credentials from Fabric Key Vault integration\")\n",
    "                return fabric_tenant, fabric_client_id, fabric_secret, True\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"[Auth] Fabric Key Vault not configured: {str(e)[:100]}\")\n",
    "            \n",
    "    except ImportError:\n",
    "        running_in_fabric = False\n",
    "    \n",
    "    # Use environment variables or parameter values\n",
    "    final_tenant = tenant_id or os.getenv(\"FABRIC_TENANT_ID\")\n",
    "    final_client_id = client_id or os.getenv(\"FABRIC_APP_ID\") \n",
    "    final_secret = client_secret_env or os.getenv(\"FABRIC_APP_SECRET\")\n",
    "    \n",
    "    if not all([final_tenant, final_client_id, final_secret]):\n",
    "        missing = []\n",
    "        if not final_tenant: missing.append(\"tenant_id/FABRIC_TENANT_ID\")\n",
    "        if not final_client_id: missing.append(\"client_id/FABRIC_APP_ID\")\n",
    "        if not final_secret: missing.append(\"client_secret/FABRIC_APP_SECRET\")\n",
    "        \n",
    "        print(f\"[Auth] ‚ùå Missing credentials: {', '.join(missing)}\")\n",
    "        if running_in_fabric:\n",
    "            print(\"[Auth] üí° In Fabric, you can:\")\n",
    "            print(\"       1. Set up Key Vault integration with secrets named 'TenantId', 'ClientId', 'ClientSecret'\")\n",
    "            print(\"       2. Set values directly in the parameters cell above\")\n",
    "            print(\"       3. Use workspace managed identity (if configured)\")\n",
    "        else:\n",
    "            print(\"[Auth] üí° Set missing values in your .env file\")\n",
    "            \n",
    "        return None, None, None, False\n",
    "    \n",
    "    auth_source = \"Fabric Key Vault\" if running_in_fabric else \"Environment Variables\"\n",
    "    print(f\"[Auth] ‚úÖ Using credentials from {auth_source}\")\n",
    "    return final_tenant, final_client_id, final_secret, running_in_fabric\n",
    "\n",
    "# Test credential availability\n",
    "print(\"üîê Testing Credential Availability:\")\n",
    "test_tenant, test_client, test_secret, is_fabric = get_credentials_fabric_aware()\n",
    "\n",
    "if all([test_tenant, test_client, test_secret]):\n",
    "    print(f\" Credentials available\")\n",
    "    print(f\" Runtime: {'Fabric' if is_fabric else 'Local'}\")\n",
    "    print(f\" Tenant: {test_tenant[:8]}...\")\n",
    "    print(f\" Client: {test_client[:8]}...\")\n",
    "    print(f\" Secret: {'***configured***'}\")\n",
    "else:\n",
    "    print(f\" Credentials missing - check configuration above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55106c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Environment Variable Validation ===\n",
    "# This cell validates that all required environment variables are properly set\n",
    "\n",
    "import os\n",
    "\n",
    "# Check if .env file exists in current directory or parent directories\n",
    "env_files_found = []\n",
    "for path in ['.env', '../.env', '../../.env']:\n",
    "    if os.path.exists(path):\n",
    "        env_files_found.append(path)\n",
    "\n",
    "print(\"üîç Environment File Detection:\")\n",
    "if env_files_found:\n",
    "    print(f\"   Found .env files: {', '.join(env_files_found)}\")\n",
    "else:\n",
    "    print(\"   No .env files found in current or parent directories\")\n",
    "    print(\"   Make sure to copy .env.example to .env and fill in your values\")\n",
    "\n",
    "# Detailed environment variable status\n",
    "required_vars = [\n",
    "    (\"FABRIC_TENANT_ID\", \"Azure tenant ID\"),\n",
    "    (\"FABRIC_APP_ID\", \"Service principal client ID\"),\n",
    "    (\"FABRIC_APP_SECRET\", \"Service principal client secret\"),\n",
    "    (\"FABRIC_WORKSPACE_ID\", \"Fabric workspace ID\"),\n",
    "    (\"DCR_ENDPOINT_HOST\", \"Data Collection Rule endpoint host\"),\n",
    "    (\"DCR_IMMUTABLE_ID\", \"Data Collection Rule immutable ID\"),\n",
    "]\n",
    "\n",
    "optional_vars = [\n",
    "    (\"LOG_ANALYTICS_WORKSPACE_ID\", \"Log Analytics workspace ID\"),\n",
    "    (\"AZURE_SUBSCRIPTION_ID\", \"Azure subscription ID\"),\n",
    "    (\"FABRIC_RUNTIME_VERSION\", \"Fabric runtime version\"),\n",
    "    (\"AZURE_KEY_VAULT_URI\", \"Azure Key Vault URI\"),\n",
    "    (\"AZURE_KEY_VAULT_SECRET_NAME\", \"Key Vault secret name\"),\n",
    "]\n",
    "\n",
    "print(\"\\nüìã Required Environment Variables:\")\n",
    "for var_name, description in required_vars:\n",
    "    value = os.getenv(var_name)\n",
    "    if value:\n",
    "        # Show first 8 chars for security\n",
    "        display_value = value[:8] + \"...\" if len(value) > 8 else value\n",
    "        print(f\"   ‚úÖ {var_name}: {display_value}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {var_name}: Not set - {description}\")\n",
    "\n",
    "print(\"\\nüìã Optional Environment Variables:\")\n",
    "for var_name, description in optional_vars:\n",
    "    value = os.getenv(var_name)\n",
    "    if value:\n",
    "        display_value = value[:8] + \"...\" if len(value) > 8 else value\n",
    "        print(f\"   ‚úÖ {var_name}: {display_value}\")\n",
    "    else:\n",
    "        print(f\"   ‚ö™ {var_name}: Not set - {description}\")\n",
    "\n",
    "print(\"\\nüí° Next Steps:\")\n",
    "print(\"   1. Ensure all required variables are set in your .env file\")\n",
    "print(\"   2. Run the parameters cell above to load configuration\")\n",
    "print(\"   3. Proceed with authentication and data collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690d2e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Fabric Runtime Detection ===\n",
    "# This cell detects if we're running in Fabric and adapts accordingly\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Detect if we're running in Fabric\n",
    "running_in_fabric = False\n",
    "try:\n",
    "    import notebookutils\n",
    "    running_in_fabric = True\n",
    "    print(\"üè≠ Running in Microsoft Fabric environment\")\n",
    "    print(f\"   Fabric notebook utilities available: {notebookutils is not None}\")\n",
    "    \n",
    "    # Try to get workspace context from Fabric\n",
    "    try:\n",
    "        # In Fabric, you can get current workspace info\n",
    "        fabric_workspace_info = notebookutils.credentials.getSecret(\"FabricWorkspace\", \"WorkspaceId\")\n",
    "        if fabric_workspace_info and not workspace_id:\n",
    "            workspace_id = fabric_workspace_info\n",
    "            print(f\"   Using Fabric workspace context: {workspace_id[:8]}...\")\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"üíª Running in local development environment\")\n",
    "    print(\"   Loading configuration from .env file\")\n",
    "\n",
    "# Check for semantic-link-sempy (available in Fabric)\n",
    "try:\n",
    "    import sempy.fabric as fabric\n",
    "    print(\"‚úÖ Semantic Link available - can use Fabric workspace functions\")\n",
    "    \n",
    "    # Get current workspace if not set\n",
    "    if not workspace_id:\n",
    "        try:\n",
    "            current_workspace = fabric.get_workspace_id()\n",
    "            if current_workspace:\n",
    "                workspace_id = current_workspace\n",
    "                print(f\"   Auto-detected workspace ID: {workspace_id[:8]}...\")\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "except ImportError:\n",
    "    if running_in_fabric:\n",
    "        print(\"‚ö†Ô∏è  Semantic Link not available in this Fabric runtime\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è  Semantic Link not available (local environment)\")\n",
    "\n",
    "# Fabric-specific authentication options\n",
    "if running_in_fabric:\n",
    "    print(\"\\nüîê Fabric Authentication Options:\")\n",
    "    print(\"   1. Use Fabric workspace identity (recommended)\")\n",
    "    print(\"   2. Use Key Vault with workspace managed identity\")\n",
    "    print(\"   3. Set credentials in parameters cell\")\n",
    "    print(\"   4. Use environment variables (if .env file uploaded)\")\n",
    "    \n",
    "    # In Fabric, you can use workspace identity for authentication\n",
    "    try:\n",
    "        # Check if we can use Fabric's built-in authentication\n",
    "        if hasattr(notebookutils, 'credentials'):\n",
    "            print(\"   ‚úÖ Fabric credential utilities available\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è  Fabric credential utilities not available\")\n",
    "    except:\n",
    "        pass\n",
    "else:\n",
    "    print(\"\\nüîê Local Development Authentication:\")\n",
    "    print(\"   Using environment variables from .env file\")\n",
    "\n",
    "print(f\"\\nüìç Current Configuration:\")\n",
    "print(f\"   Runtime Environment: {'Fabric' if running_in_fabric else 'Local'}\")\n",
    "print(f\"   Workspace ID: {workspace_id[:8] + '...' if workspace_id else 'Not set'}\")\n",
    "print(f\"   Python Version: {sys.version.split()[0]}\")\n",
    "print(f\"   Working Directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2ce670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Verify critical imports ===\n",
    "try:\n",
    "    from azure.identity import DefaultAzureCredential\n",
    "    from azure.keyvault.secrets import SecretClient\n",
    "    import msal\n",
    "    import pandas as pd\n",
    "    import requests\n",
    "\n",
    "    print(\"\\n‚úÖ All critical imports successful - environment ready!\")\n",
    "except ImportError as e:\n",
    "    print(f\"\\n‚ùå Import error: {e}\")\n",
    "    print(\"Some packages may need manual installation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b243b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === define main functions ===\n",
    "import os, json, time, datetime as dt\n",
    "import requests\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "# Key Vault helper with multiple authentication options\n",
    "def get_secret_from_kv(\n",
    "    vault_uri: str,\n",
    "    secret_name: str,\n",
    "    tenant_id: str = None,\n",
    "    client_id: str = None,\n",
    "    client_secret: str = None,\n",
    "    use_managed_identity: bool = False,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Get secret from Key Vault using either:\n",
    "    1. Managed Identity (recommended for Azure resources)\n",
    "    2. Client credentials (service principal)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from azure.keyvault.secrets import SecretClient\n",
    "\n",
    "        if use_managed_identity:\n",
    "            # Use managed identity (no secrets needed!)\n",
    "            from azure.identity import ManagedIdentityCredential\n",
    "\n",
    "            print(\"[KeyVault] Using managed identity authentication\")\n",
    "            credential = ManagedIdentityCredential()\n",
    "        else:\n",
    "            # Use client credentials (requires secret - circular dependency)\n",
    "            from azure.identity import ClientSecretCredential\n",
    "\n",
    "            print(\"[KeyVault] Using client secret authentication\")\n",
    "            credential = ClientSecretCredential(\n",
    "                tenant_id=tenant_id, client_id=client_id, client_secret=client_secret\n",
    "            )\n",
    "\n",
    "        client = SecretClient(vault_url=vault_uri, credential=credential)\n",
    "        secret = client.get_secret(secret_name)\n",
    "        return secret.value\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[KeyVault] Failed to fetch secret '{secret_name}': {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Tokens\n",
    "FABRIC_SCOPE = \"https://api.fabric.microsoft.com/.default\"\n",
    "MONITOR_SCOPE = \"https://monitor.azure.com/.default\"\n",
    "FABRIC_API = \"https://api.fabric.microsoft.com/v1\"\n",
    "\n",
    "\n",
    "def acquire_token_client_credentials(\n",
    "    tenant: str, client_id: str, client_secret: str, scope: str\n",
    ") -> str:\n",
    "    import msal\n",
    "\n",
    "    authority = f\"https://login.microsoftonline.com/{tenant}\"\n",
    "    app = msal.ConfidentialClientApplication(\n",
    "        client_id, authority=authority, client_credential=client_secret\n",
    "    )\n",
    "    result = app.acquire_token_for_client(scopes=[scope])\n",
    "    if \"access_token\" not in result:\n",
    "        print(f\"‚ùå Token acquisition failed for {scope}\")\n",
    "        print(f\"   Error: {result.get('error', 'Unknown error')}\")\n",
    "        print(f\"   Description: {result.get('error_description', 'No description')}\")\n",
    "        raise RuntimeError(f\"Failed to get token for {scope}: {result}\")\n",
    "\n",
    "    # Debug: Show token info (first/last 10 chars only)\n",
    "    token = result[\"access_token\"]\n",
    "    print(f\"‚úÖ Token acquired for {scope}: {token[:10]}...{token[-10:]}\")\n",
    "    return token\n",
    "\n",
    "\n",
    "def acquire_token_managed_identity(scope: str) -> str:\n",
    "    \"\"\"Get token using managed identity (for Azure resources)\"\"\"\n",
    "    try:\n",
    "        from azure.identity import ManagedIdentityCredential\n",
    "\n",
    "        credential = ManagedIdentityCredential()\n",
    "        token = credential.get_token(scope)\n",
    "        print(\n",
    "            f\"‚úÖ Managed identity token acquired for {scope}: {token.token[:10]}...{token.token[-10:]}\"\n",
    "        )\n",
    "        return token.token\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to get managed identity token for {scope}: {e}\")\n",
    "\n",
    "\n",
    "def iso_now() -> str:\n",
    "    return (\n",
    "        dt.datetime.utcnow()\n",
    "        .replace(tzinfo=dt.timezone.utc)\n",
    "        .isoformat()\n",
    "        .replace(\"+00:00\", \"Z\")\n",
    "    )\n",
    "\n",
    "\n",
    "def to_iso(ts: dt.datetime) -> str:\n",
    "    if ts.tzinfo is None:\n",
    "        ts = ts.replace(tzinfo=dt.timezone.utc)\n",
    "    return ts.isoformat().replace(\"+00:00\", \"Z\")\n",
    "\n",
    "\n",
    "def parse_iso(s: str) -> dt.datetime:\n",
    "    if not s:\n",
    "        return None\n",
    "    if s.endswith(\"Z\"):\n",
    "        s = s[:-1] + \"+00:00\"\n",
    "    parsed = dt.datetime.fromisoformat(s)\n",
    "    # Ensure timezone awareness - if no timezone, assume UTC\n",
    "    if parsed.tzinfo is None:\n",
    "        parsed = parsed.replace(tzinfo=dt.timezone.utc)\n",
    "    return parsed\n",
    "\n",
    "\n",
    "def within_lookback(start_iso: str, end_iso: str, lookback_minutes: int) -> bool:\n",
    "    edge = dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc) - dt.timedelta(\n",
    "        minutes=int(lookback_minutes)\n",
    "    )\n",
    "    t = parse_iso(end_iso) or parse_iso(start_iso)\n",
    "    return (t is not None) and (t >= edge)\n",
    "\n",
    "\n",
    "# Fabric REST with better error handling\n",
    "def list_item_job_instances(\n",
    "    workspace_id: str, item_id: str, token: str\n",
    ") -> List[Dict[str, Any]]:\n",
    "    url = f\"{FABRIC_API}/workspaces/{workspace_id}/items/{item_id}/jobs/instances\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    out = []\n",
    "\n",
    "    print(f\"[DEBUG] Calling: {url}\")\n",
    "    print(f\"[DEBUG] Token (first 20 chars): {token[:20]}...\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            r = requests.get(url, headers=headers, timeout=60)\n",
    "            print(f\"[DEBUG] Response status: {r.status_code}\")\n",
    "\n",
    "            if r.status_code == 401:\n",
    "                print(\"‚ùå 401 Unauthorized - Authentication failed\")\n",
    "                print(\"Possible causes:\")\n",
    "                print(\"1. Token expired or invalid\")\n",
    "                print(\"2. Service principal doesn't have 'Fabric.ReadAll' permission\")\n",
    "                print(\"3. Service principal not granted admin consent\")\n",
    "                print(\"4. Wrong tenant ID\")\n",
    "                print(f\"Response: {r.text}\")\n",
    "\n",
    "            elif r.status_code == 403:\n",
    "                print(\"‚ùå 403 Forbidden - Permission denied\")\n",
    "                print(\n",
    "                    \"The service principal needs 'Fabric.ReadAll' application permission\"\n",
    "                )\n",
    "                print(f\"Response: {r.text}\")\n",
    "\n",
    "            elif r.status_code == 404:\n",
    "                print(\"‚ùå 404 Not Found - Workspace or item doesn't exist\")\n",
    "                print(f\"Workspace ID: {workspace_id}\")\n",
    "                print(f\"Item ID: {item_id}\")\n",
    "                print(f\"Response: {r.text}\")\n",
    "\n",
    "            r.raise_for_status()\n",
    "            data = r.json()\n",
    "            out.extend(data.get(\"value\", []))\n",
    "            next_link = data.get(\"nextLink\")\n",
    "            if not next_link:\n",
    "                break\n",
    "            url = next_link\n",
    "\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            print(f\"‚ùå HTTP Error: {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Unexpected error: {e}\")\n",
    "            raise\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def query_pipeline_activity_runs(\n",
    "    workspace_id: str,\n",
    "    job_instance_id: str,\n",
    "    token: str,\n",
    "    last_after_iso: str,\n",
    "    last_before_iso: str,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    url = f\"{FABRIC_API}/workspaces/{workspace_id}/datapipelines/pipelineruns/{job_instance_id}/queryactivityruns\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n",
    "    body = {\n",
    "        \"filters\": [],\n",
    "        \"orderBy\": [{\"orderBy\": \"ActivityRunStart\", \"order\": \"DESC\"}],\n",
    "        \"lastUpdatedAfter\": last_after_iso,\n",
    "        \"lastUpdatedBefore\": last_before_iso,\n",
    "    }\n",
    "    r = requests.post(url, headers=headers, json=body, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    return data.get(\"value\") or data.get(\"activityRuns\") or data.get(\"items\") or []\n",
    "\n",
    "\n",
    "# Logs Ingestion API\n",
    "def post_rows_to_dcr(\n",
    "    endpoint_host: str,\n",
    "    dcr_id: str,\n",
    "    stream_name: str,\n",
    "    rows: List[Dict[str, Any]],\n",
    "    monitor_token: str,\n",
    "):\n",
    "    if not rows:\n",
    "        return {\"sent\": 0, \"batches\": 0}\n",
    "    import json\n",
    "\n",
    "    MAX_BYTES = 950_000  # keep under API limit (~1MB)\n",
    "    batch, batches, sent, size = [], 0, 0, 2  # 2 for []\n",
    "\n",
    "    def flush():\n",
    "        nonlocal batches, sent, batch, size\n",
    "        if not batch:\n",
    "            return\n",
    "        url = f\"https://{endpoint_host}/dataCollectionRules/{dcr_id}/streams/{stream_name}?api-version=2023-01-01\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {monitor_token}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        }\n",
    "        resp = requests.post(url, headers=headers, data=json.dumps(batch), timeout=60)\n",
    "        if resp.status_code >= 400:\n",
    "            raise RuntimeError(\n",
    "                f\"Ingestion failed ({resp.status_code}): {resp.text[:500]}\"\n",
    "            )\n",
    "        batches += 1\n",
    "        sent += len(batch)\n",
    "        batch, size = [], 2\n",
    "\n",
    "    for row in rows:\n",
    "        s = len(json.dumps(row, separators=(\",\", \":\")))\n",
    "        if size + s + (1 if batch else 0) > MAX_BYTES:\n",
    "            flush()\n",
    "        batch.append(row)\n",
    "        size += s + (1 if batch else 0)\n",
    "    flush()\n",
    "    return {\"sent\": sent, \"batches\": batches}\n",
    "\n",
    "\n",
    "# Mappers\n",
    "def map_pipeline_run(\n",
    "    workspace_id: str, item_id: str, run: Dict[str, Any]\n",
    ") -> Dict[str, Any]:\n",
    "    st, et = run.get(\"startTimeUtc\"), run.get(\"endTimeUtc\")\n",
    "    dur_ms = None\n",
    "    try:\n",
    "        if st and et:\n",
    "            dur_ms = int((parse_iso(et) - parse_iso(st)).total_seconds() * 1000)\n",
    "    except Exception:\n",
    "        pass\n",
    "    fr = run.get(\"failureReason\") or {}\n",
    "    return {\n",
    "        \"TimeGenerated\": et or st or iso_now(),\n",
    "        \"WorkspaceId\": workspace_id,\n",
    "        \"ItemId\": item_id,\n",
    "        \"ItemType\": \"DataPipeline\",\n",
    "        \"RunId\": run.get(\"id\"),\n",
    "        \"JobType\": run.get(\"jobType\", \"Pipeline\"),\n",
    "        \"InvokeType\": run.get(\"invokeType\"),\n",
    "        \"Status\": run.get(\"status\"),\n",
    "        \"StartTimeUtc\": st,\n",
    "        \"EndTimeUtc\": et,\n",
    "        \"DurationMs\": dur_ms,\n",
    "        \"FailureCode\": fr.get(\"errorCode\") if isinstance(fr, dict) else None,\n",
    "        \"FailureMessage\": fr.get(\"message\") if isinstance(fr, dict) else None,\n",
    "    }\n",
    "\n",
    "\n",
    "def map_activity_run(\n",
    "    workspace_id: str, pipeline_id: str, pipeline_run_id: str, act: Dict[str, Any]\n",
    ") -> Dict[str, Any]:\n",
    "    st = act.get(\"activityRunStart\") or act.get(\"ActivityRunStart\")\n",
    "    et = act.get(\"activityRunEnd\") or act.get(\"ActivityRunEnd\")\n",
    "    dur_ms = act.get(\"durationInMs\") or act.get(\"DurationInMs\")\n",
    "    \n",
    "    # Extract performance metrics if available\n",
    "    rows_read = None\n",
    "    rows_written = None\n",
    "    \n",
    "    # Try to get performance metrics from activity output\n",
    "    output = act.get(\"output\") or {}\n",
    "    if isinstance(output, dict):\n",
    "        # Check for common performance metric fields\n",
    "        rows_read = output.get(\"rowsRead\") or output.get(\"dataRead\") or output.get(\"recordsRead\")\n",
    "        rows_written = output.get(\"rowsWritten\") or output.get(\"dataWritten\") or output.get(\"recordsWritten\")\n",
    "    \n",
    "    return {\n",
    "        \"TimeGenerated\": et or st or iso_now(),\n",
    "        \"WorkspaceId\": workspace_id,\n",
    "        \"PipelineName\": pipeline_id,  # This might be the pipeline name, not ID - adjust as needed\n",
    "        \"ActivityName\": act.get(\"activityName\") or act.get(\"ActivityName\"),\n",
    "        \"ActivityType\": act.get(\"activityType\") or act.get(\"ActivityType\"),\n",
    "        \"RunId\": pipeline_run_id,\n",
    "        \"Status\": act.get(\"status\") or act.get(\"Status\"),\n",
    "        \"StartTimeUtc\": st,\n",
    "        \"EndTimeUtc\": et,\n",
    "        \"DurationMs\": dur_ms,\n",
    "        \"RowsRead\": rows_read,\n",
    "        \"RowsWritten\": rows_written,\n",
    "        \"ErrorCode\": (\n",
    "            (act.get(\"error\") or {}).get(\"code\")\n",
    "            if isinstance(act.get(\"error\"), dict)\n",
    "            else None\n",
    "        ),\n",
    "        \"ErrorMessage\": (\n",
    "            (act.get(\"error\") or {}).get(\"message\")\n",
    "            if isinstance(act.get(\"error\"), dict)\n",
    "            else None\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "def map_dataflow_run(\n",
    "    workspace_id: str, item_id: str, run: Dict[str, Any]\n",
    ") -> Dict[str, Any]:\n",
    "    st, et = run.get(\"startTimeUtc\"), run.get(\"endTimeUtc\")\n",
    "    dur_ms = None\n",
    "    try:\n",
    "        if st and et:\n",
    "            dur_ms = int((parse_iso(et) - parse_iso(st)).total_seconds() * 1000)\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # Handle error information - prefer simple error message for dataflows\n",
    "    fr = run.get(\"failureReason\") or {}\n",
    "    error_message = None\n",
    "    if isinstance(fr, dict):\n",
    "        error_message = fr.get(\"message\")\n",
    "    elif isinstance(fr, str):\n",
    "        error_message = fr\n",
    "    \n",
    "    return {\n",
    "        \"TimeGenerated\": et or st or iso_now(),\n",
    "        \"WorkspaceId\": workspace_id,\n",
    "        \"DataflowId\": item_id,  # Using DataflowId to match table schema\n",
    "        \"RunId\": run.get(\"id\"),\n",
    "        \"Status\": run.get(\"status\"),\n",
    "        \"StartTimeUtc\": st,\n",
    "        \"EndTimeUtc\": et,\n",
    "        \"DurationMs\": dur_ms,\n",
    "        \"ErrorMessage\": error_message,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a236d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Main ===\n",
    "\n",
    "client_secret = None\n",
    "\n",
    "# Option 1: Direct environment variable (simplest)\n",
    "if client_secret_env:\n",
    "    client_secret = client_secret_env\n",
    "    print(\"‚úÖ Using client secret from environment variable\")\n",
    "\n",
    "# Option 2: Key Vault authentication\n",
    "elif use_key_vault:\n",
    "    print(f\"Fetching secret '{key_vault_secret_name}' from Key Vault '{key_vault_uri}'...\")\n",
    "    \n",
    "    if use_managed_identity:\n",
    "        # Use managed identity to access Key Vault\n",
    "        print(\"üîê Using managed identity to authenticate to Key Vault\")\n",
    "        client_secret = get_secret_from_kv(\n",
    "            key_vault_uri, \n",
    "            key_vault_secret_name, \n",
    "            use_managed_identity=True\n",
    "        )\n",
    "    else:\n",
    "        # Use client credentials to access Key Vault\n",
    "        print(\"üîê Using client credentials to authenticate to Key Vault\")\n",
    "        temp_secret = os.getenv(\"FABRIC_APP_SECRET\")\n",
    "        if not temp_secret:\n",
    "            print(\"‚ùå Cannot access Key Vault with client credentials: FABRIC_APP_SECRET environment variable is required\")\n",
    "            print(\"Solutions:\")\n",
    "            print(\"1. Set FABRIC_APP_SECRET environment variable\")\n",
    "            print(\"2. Set use_managed_identity=True (if running on Azure)\")\n",
    "            print(\"3. Set use_key_vault=False and use environment variables directly\")\n",
    "            raise RuntimeError(\"FABRIC_APP_SECRET required for Key Vault client credential authentication\")\n",
    "        \n",
    "        client_secret = get_secret_from_kv(\n",
    "            key_vault_uri, \n",
    "            key_vault_secret_name, \n",
    "            tenant_id, \n",
    "            client_id, \n",
    "            temp_secret,\n",
    "            use_managed_identity=False\n",
    "        )\n",
    "    \n",
    "    print(f\"Key Vault returned: {'***' if client_secret else 'None'}\")\n",
    "\n",
    "# Final validation\n",
    "if not client_secret:\n",
    "    print(\"‚ùå No client secret found!\")\n",
    "    print(\"Authentication Configuration:\")\n",
    "    print(f\"  use_key_vault: {use_key_vault}\")\n",
    "    print(f\"  use_managed_identity: {use_managed_identity}\")\n",
    "    print(f\"  FABRIC_APP_SECRET env var: {'Set' if client_secret_env else 'Not set'}\")\n",
    "    print()\n",
    "    print(\"Solutions:\")\n",
    "    print(\"1. Set FABRIC_APP_SECRET environment variable\")\n",
    "    print(\"2. Create .env file with FABRIC_APP_SECRET=your-secret\")\n",
    "    print(\"3. Use managed identity (set use_managed_identity=True)\")\n",
    "    print(\"4. Set use_key_vault=False and provide secret directly\")\n",
    "    raise RuntimeError(\"Client secret not found. Check your authentication configuration.\")\n",
    "\n",
    "print(\"‚úÖ Client secret resolved successfully\")\n",
    "\n",
    "# Determine token acquisition method\n",
    "use_managed_identity_for_tokens = use_managed_identity and use_key_vault\n",
    "\n",
    "if use_managed_identity_for_tokens:\n",
    "    print(\"üîê Using managed identity for token acquisition\")\n",
    "    try:\n",
    "        print(\"Acquiring Fabric API token with managed identity...\")\n",
    "        fabric_token = acquire_token_managed_identity(FABRIC_SCOPE)\n",
    "        \n",
    "        print(\"Acquiring Azure Monitor token with managed identity...\")\n",
    "        monitor_token = acquire_token_managed_identity(MONITOR_SCOPE)\n",
    "        \n",
    "        print(\"‚úÖ All tokens acquired successfully with managed identity\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Managed identity token acquisition failed: {e}\")\n",
    "        print(\"Falling back to client credentials...\")\n",
    "        use_managed_identity_for_tokens = False\n",
    "\n",
    "if not use_managed_identity_for_tokens:\n",
    "    print(\"üîê Using client credentials for token acquisition\")\n",
    "    # Acquire tokens using client credentials\n",
    "    print(\"Acquiring Fabric API token...\")\n",
    "    fabric_token = acquire_token_client_credentials(\n",
    "        tenant_id, client_id, client_secret, FABRIC_SCOPE\n",
    "    )\n",
    "\n",
    "    print(\"Acquiring Azure Monitor token...\")\n",
    "    monitor_token = acquire_token_client_credentials(\n",
    "        tenant_id, client_id, client_secret, MONITOR_SCOPE\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ All tokens acquired successfully with client credentials\")\n",
    "\n",
    "now = dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc)\n",
    "window_start = to_iso(now - dt.timedelta(minutes=int(lookback_minutes)))\n",
    "window_end = to_iso(now)\n",
    "\n",
    "print(f\"Collecting data from {window_start} to {window_end} (lookback: {lookback_minutes} minutes)\")\n",
    "\n",
    "pipeline_rows, activity_rows, dataflow_rows = [], [], []\n",
    "\n",
    "# Pipelines\n",
    "print(f\"Processing {len(pipeline_item_ids)} pipelines...\")\n",
    "for pid in pipeline_item_ids:\n",
    "    print(f\"  Fetching runs for pipeline {pid}\")\n",
    "    runs = list_item_job_instances(workspace_id, pid, fabric_token)\n",
    "    runs_recent = [\n",
    "        r\n",
    "        for r in runs\n",
    "        if within_lookback(r.get(\"startTimeUtc\"), r.get(\"endTimeUtc\"), lookback_minutes)\n",
    "    ]\n",
    "    print(f\"    Found {len(runs_recent)} recent runs\")\n",
    "    for r in runs_recent:\n",
    "        pipeline_rows.append(map_pipeline_run(workspace_id, pid, r))\n",
    "        if collect_activity_runs:\n",
    "            run_id = r.get(\"id\")\n",
    "            try:\n",
    "                acts = query_pipeline_activity_runs(\n",
    "                    workspace_id, run_id, fabric_token, window_start, window_end\n",
    "                )\n",
    "                print(f\"    Found {len(acts)} activity runs for run {run_id}\")\n",
    "                for a in acts:\n",
    "                    activity_rows.append(map_activity_run(workspace_id, pid, run_id, a))\n",
    "            except Exception as e:\n",
    "                print(f\"    ‚ö†Ô∏è  Failed to get activity runs for pipeline {pid} run {run_id}: {e}\")\n",
    "\n",
    "# Dataflows\n",
    "print(f\"Processing {len(dataflow_item_ids)} dataflows...\")\n",
    "for did in dataflow_item_ids:\n",
    "    print(f\"  Fetching runs for dataflow {did}\")\n",
    "    runs = list_item_job_instances(workspace_id, did, fabric_token)\n",
    "    runs_recent = [\n",
    "        r\n",
    "        for r in runs\n",
    "        if within_lookback(r.get(\"startTimeUtc\"), r.get(\"endTimeUtc\"), lookback_minutes)\n",
    "    ]\n",
    "    print(f\"    Found {len(runs_recent)} recent runs\")\n",
    "    for r in runs_recent:\n",
    "        dataflow_rows.append(map_dataflow_run(workspace_id, did, r))\n",
    "\n",
    "# Ingest to LA via Logs Ingestion API\n",
    "print(f\"\\nIngesting data to Log Analytics:\")\n",
    "print(f\"  Pipeline runs: {len(pipeline_rows)}\")\n",
    "print(f\"  Activity runs: {len(activity_rows)}\")\n",
    "print(f\"  Dataflow runs: {len(dataflow_rows)}\")\n",
    "\n",
    "summary = {}\n",
    "if pipeline_rows:\n",
    "    print(\"  Sending pipeline runs...\")\n",
    "    res = post_rows_to_dcr(\n",
    "        dcr_endpoint_host,\n",
    "        dcr_immutable_id,\n",
    "        stream_pipeline,\n",
    "        pipeline_rows,\n",
    "        monitor_token,\n",
    "    )\n",
    "    summary[\"pipeline_rows\"] = res\n",
    "\n",
    "if activity_rows:\n",
    "    print(\"  Sending activity runs...\")\n",
    "    res = post_rows_to_dcr(\n",
    "        dcr_endpoint_host,\n",
    "        dcr_immutable_id,\n",
    "        stream_activity,\n",
    "        activity_rows,\n",
    "        monitor_token,\n",
    "    )\n",
    "    summary[\"activity_rows\"] = res\n",
    "\n",
    "if dataflow_rows:\n",
    "    print(\"  Sending dataflow runs...\")\n",
    "    res = post_rows_to_dcr(\n",
    "        dcr_endpoint_host,\n",
    "        dcr_immutable_id,\n",
    "        stream_dataflow,\n",
    "        dataflow_rows,\n",
    "        monitor_token,\n",
    "    )\n",
    "    summary[\"dataflow_rows\"] = res\n",
    "\n",
    "import json\n",
    "\n",
    "print(\"\\n‚úÖ Done! Ingestion summary:\")\n",
    "print(json.dumps(summary, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ab05ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Troubleshooting: Why are Log Analytics tables empty? ===\n",
    "\n",
    "print(\"üîç TROUBLESHOOTING LOG ANALYTICS TABLES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Check if data was actually sent\n",
    "print(\"\\n1. DATA COLLECTION SUMMARY:\")\n",
    "if 'summary' in locals():\n",
    "    import json\n",
    "    print(json.dumps(summary, indent=2))\n",
    "    \n",
    "    total_sent = sum(result.get('sent', 0) for result in summary.values())\n",
    "    print(f\"\\nTotal records sent: {total_sent}\")\n",
    "    \n",
    "    if total_sent == 0:\n",
    "        print(\"‚ùå No data was sent - this explains why tables are empty!\")\n",
    "        print(\"Possible causes:\")\n",
    "        print(\"   - No recent runs in the lookback window\")\n",
    "        print(\"   - Empty item ID lists\")\n",
    "        print(\"   - API authentication issues\")\n",
    "    else:\n",
    "        print(f\"‚úÖ {total_sent} records were successfully sent to DCR\")\n",
    "else:\n",
    "    print(\"‚ùå No summary data available - collection may have failed\")\n",
    "\n",
    "# 2. Check ingestion delay\n",
    "print(\"\\n2. LOG ANALYTICS INGESTION TIMING:\")\n",
    "print(\"   Data ingestion to Log Analytics can take 5-30 minutes\")\n",
    "print(\"   Custom tables may take longer for the first ingestion\")\n",
    "print(f\"   Data was sent at: {dt.datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC\")\n",
    "print(\"   Check tables again in 10-15 minutes\")\n",
    "\n",
    "# 3. Check table names and structure\n",
    "print(\"\\n3. LOG ANALYTICS TABLE VERIFICATION:\")\n",
    "print(\"   Expected tables in your Log Analytics workspace:\")\n",
    "print(f\"   - FabricPipelineRun_CL (should have {len(pipeline_rows)} records)\")\n",
    "print(f\"   - FabricPipelineActivityRun_CL (should have {len(activity_rows)} records)\")\n",
    "print(f\"   - FabricDataflowRun_CL (should have {len(dataflow_rows)} records)\")\n",
    "print()\n",
    "print(\"   Sample KQL queries to check tables:\")\n",
    "print(\"   ```kql\")\n",
    "print(\"   // Check if tables exist and have data\")\n",
    "print(\"   FabricPipelineRun_CL | count\")\n",
    "print(\"   FabricPipelineActivityRun_CL | count\") \n",
    "print(\"   FabricDataflowRun_CL | count\")\n",
    "print()\n",
    "print(\"   // Check recent data (last 24 hours)\")\n",
    "print(\"   FabricPipelineRun_CL | where TimeGenerated > ago(24h)\")\n",
    "print(\"   FabricDataflowRun_CL | where TimeGenerated > ago(24h)\")\n",
    "print(\"   ```\")\n",
    "\n",
    "# 4. Check DCR configuration\n",
    "print(\"\\n4. DCR CONFIGURATION CHECK:\")\n",
    "print(f\"   DCR Endpoint: {dcr_endpoint_host}\")\n",
    "print(f\"   DCR Immutable ID: {dcr_immutable_id[:20]}...\")\n",
    "print(f\"   Stream Names Used:\")\n",
    "print(f\"     - {stream_pipeline}\")\n",
    "print(f\"     - {stream_activity}\")\n",
    "print(f\"     - {stream_dataflow}\")\n",
    "\n",
    "# 5. Sample data inspection\n",
    "print(\"\\n5. SAMPLE DATA INSPECTION:\")\n",
    "if pipeline_rows:\n",
    "    print(\"   Sample Pipeline Run Data:\")\n",
    "    sample_pipeline = pipeline_rows[0]\n",
    "    for key, value in sample_pipeline.items():\n",
    "        print(f\"     {key}: {value}\")\n",
    "    print()\n",
    "\n",
    "if dataflow_rows:\n",
    "    print(\"   Sample Dataflow Run Data:\")\n",
    "    sample_dataflow = dataflow_rows[0]\n",
    "    for key, value in sample_dataflow.items():\n",
    "        print(f\"     {key}: {value}\")\n",
    "    print()\n",
    "\n",
    "# 6. Common troubleshooting steps\n",
    "print(\"\\n6. COMMON TROUBLESHOOTING STEPS:\")\n",
    "print(\"   a) Wait 10-15 minutes for ingestion to complete\")\n",
    "print(\"   b) Check if tables were created properly in Log Analytics\")\n",
    "print(\"   c) Verify DCR is properly configured and linked to workspace\")\n",
    "print(\"   d) Check if service principal has Log Analytics Contributor role\")\n",
    "print(\"   e) Verify workspace ID matches your actual Log Analytics workspace\")\n",
    "print(\"   f) Check Azure Activity Log for any DCR ingestion errors\")\n",
    "\n",
    "print(\"\\n7. IMMEDIATE ACTIONS TO VERIFY:\")\n",
    "print(\"   1. Go to your Log Analytics workspace in Azure Portal\")\n",
    "print(\"   2. Navigate to 'Logs' section\")\n",
    "print(\"   3. Run: search '*' | where TimeGenerated > ago(1h) | take 10\")\n",
    "print(\"   4. Check if any FabricPipeline* or FabricDataflow* tables appear\")\n",
    "print(\"   5. If tables don't exist, check if Bicep/Terraform deployment completed successfully\")\n",
    "\n",
    "print(\"\\nüìä NEXT STEPS:\")\n",
    "if total_sent > 0:\n",
    "    print(\"   ‚úÖ Data was sent successfully - wait 10-15 minutes and check again\")\n",
    "    print(\"   ‚úÖ If still empty after 30 minutes, check DCR configuration and permissions\")\n",
    "else:\n",
    "    print(\"   ‚ùå No data was sent - investigate why collections returned empty results\")\n",
    "    print(\"   ‚ùå Check pipeline/dataflow IDs and workspace ID configuration\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
