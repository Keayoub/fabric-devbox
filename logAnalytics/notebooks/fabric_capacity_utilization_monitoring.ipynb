{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "624e79fc",
   "metadata": {},
   "source": [
    "# Fabric Capacity Utilization Monitoring\n",
    "\n",
    "**Production monitoring solution** that collects capacity metrics and workload data from Microsoft Fabric REST APIs and sends to Azure Log Analytics.\n",
    "\n",
    "## What This Monitors\n",
    "- âœ… **Capacity Inventory**: Basic capacity details, workspace counts, item distribution\n",
    "- âœ… **Workload Data**: Item-based workload tracking across all workspaces\n",
    "- âœ… **Workspace Events**: Workspace lifecycle and item details\n",
    "\n",
    "## Data Streams\n",
    "- `Custom-FabricCapacityMetrics_CL` - Capacity utilization metrics\n",
    "- `Custom-FabricCapacityWorkloads_CL` - Workload and item inventory\n",
    "- `Custom-FabricWorkspaceEvents_CL` - Workspace events and changes\n",
    "- `Custom-FabricItemDetails_CL` - Enhanced item metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620a2bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install --quiet msal requests azure-identity azure-keyvault-secrets python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d1e20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Parameters\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Capacity Configuration\n",
    "capacity_ids = [\"95e3bf4e-6e0e-427d-9c78-88823f6d507c\"]  # Leave empty for auto-discovery\n",
    "\n",
    "# Azure Log Analytics Configuration (from your Terraform/Bicep deployment)\n",
    "dcr_endpoint_host = os.getenv(\"DCR_ENDPOINT_HOST\")\n",
    "dcr_immutable_id = os.getenv(\"DCR_IMMUTABLE_ID\")\n",
    "stream_capacity_metrics = \"Custom-FabricCapacityMetrics_CL\"\n",
    "stream_capacity_workloads = \"Custom-FabricCapacityWorkloads_CL\"\n",
    "stream_workspace_events = \"Custom-FabricWorkspaceEvents_CL\"\n",
    "stream_item_details = \"Custom-FabricItemDetails_CL\"\n",
    "\n",
    "# Authentication Configuration\n",
    "tenant_id = os.getenv(\"FABRIC_TENANT_ID\")\n",
    "client_id = os.getenv(\"FABRIC_APP_ID\")\n",
    "client_secret = os.getenv(\"FABRIC_APP_SECRET\")\n",
    "\n",
    "# Key Vault Configuration (optional)\n",
    "use_key_vault = False\n",
    "key_vault_uri = os.getenv(\"AZURE_KEY_VAULT_URI\", \"https://kaydemokeyvault.vault.azure.net/\")\n",
    "key_vault_secret_name = os.getenv(\"AZURE_KEY_VAULT_SECRET_NAME\", \"FabricServicePrincipal\")\n",
    "\n",
    "# Validation\n",
    "required_vars = [tenant_id, client_id, dcr_endpoint_host, dcr_immutable_id]\n",
    "if not client_secret and not use_key_vault:\n",
    "    required_vars.append(client_secret)\n",
    "\n",
    "if not all(required_vars):\n",
    "    missing = []\n",
    "    if not tenant_id: missing.append(\"FABRIC_TENANT_ID\")\n",
    "    if not client_id: missing.append(\"FABRIC_APP_ID\")\n",
    "    if not client_secret and not use_key_vault: missing.append(\"FABRIC_APP_SECRET\")\n",
    "    if not dcr_endpoint_host: missing.append(\"DCR_ENDPOINT_HOST\")\n",
    "    if not dcr_immutable_id: missing.append(\"DCR_IMMUTABLE_ID\")\n",
    "    raise ValueError(f\"Missing required environment variables: {', '.join(missing)}\")\n",
    "\n",
    "print(\"âœ… Configuration loaded successfully\")\n",
    "print(f\"ðŸ“Š Monitoring: {'Specific capacities' if capacity_ids else 'All accessible capacities'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3923b4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Functions\n",
    "import json\n",
    "import datetime as dt\n",
    "import requests\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "FABRIC_SCOPE = \"https://api.fabric.microsoft.com/.default\"\n",
    "MONITOR_SCOPE = \"https://monitor.azure.com/.default\"\n",
    "FABRIC_API = \"https://api.fabric.microsoft.com/v1\"\n",
    "\n",
    "def acquire_token(tenant: str, client_id: str, client_secret: str, scope: str) -> str:\n",
    "    \"\"\"Acquire OAuth token for API access\"\"\"\n",
    "    import msal\n",
    "    authority = f\"https://login.microsoftonline.com/{tenant}\"\n",
    "    app = msal.ConfidentialClientApplication(client_id, authority=authority, client_credential=client_secret)\n",
    "    result = app.acquire_token_for_client(scopes=[scope])\n",
    "    if \"access_token\" not in result:\n",
    "        raise RuntimeError(f\"Failed to acquire token: {result}\")\n",
    "    return result[\"access_token\"]\n",
    "\n",
    "def get_secret_from_key_vault(vault_uri: str, secret_name: str) -> str:\n",
    "    \"\"\"Get secret from Azure Key Vault using managed identity\"\"\"\n",
    "    from azure.keyvault.secrets import SecretClient\n",
    "    from azure.identity import DefaultAzureCredential\n",
    "    credential = DefaultAzureCredential()\n",
    "    client = SecretClient(vault_url=vault_uri, credential=credential)\n",
    "    return client.get_secret(secret_name).value\n",
    "\n",
    "def iso_now() -> str:\n",
    "    \"\"\"Get current timestamp in ISO format\"\"\"\n",
    "    return dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc).isoformat().replace(\"+00:00\", \"Z\")\n",
    "\n",
    "def post_to_log_analytics(endpoint_host: str, dcr_id: str, stream_name: str, data: List[Dict], token: str) -> Dict:\n",
    "    \"\"\"Send data to Azure Log Analytics via Data Collection Rule\"\"\"\n",
    "    if not data:\n",
    "        return {\"sent\": 0, \"batches\": 0}\n",
    "    \n",
    "    url = f\"https://{endpoint_host}/dataCollectionRules/{dcr_id}/streams/{stream_name}?api-version=2023-01-01\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\", \n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    # Split large payloads into batches (max 1MB per request)\n",
    "    MAX_BYTES = 950_000\n",
    "    batches = []\n",
    "    current_batch = []\n",
    "    current_size = 2  # Start with array brackets\n",
    "    \n",
    "    for record in data:\n",
    "        record_size = len(json.dumps(record, separators=(\",\", \":\")))\n",
    "        if current_size + record_size + (1 if current_batch else 0) > MAX_BYTES:\n",
    "            if current_batch:\n",
    "                batches.append(current_batch)\n",
    "                current_batch = []\n",
    "                current_size = 2\n",
    "        current_batch.append(record)\n",
    "        current_size += record_size + (1 if len(current_batch) > 1 else 0)\n",
    "    \n",
    "    if current_batch:\n",
    "        batches.append(current_batch)\n",
    "    \n",
    "    # Send each batch\n",
    "    total_sent = 0\n",
    "    for i, batch in enumerate(batches, 1):\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(batch), timeout=60)\n",
    "        if response.status_code >= 400:\n",
    "            raise RuntimeError(f\"Batch {i} failed ({response.status_code}): {response.text[:200]}\")\n",
    "        total_sent += len(batch)\n",
    "    \n",
    "    return {\"sent\": total_sent, \"batches\": len(batches)}\n",
    "\n",
    "print(\"âœ… Core functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bf0d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collection Functions\n",
    "\n",
    "def get_capacities(token: str) -> List[Dict]:\n",
    "    \"\"\"Get all accessible Fabric capacities\"\"\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    response = requests.get(f\"{FABRIC_API}/capacities\", headers=headers, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    return response.json().get(\"value\", [])\n",
    "\n",
    "def get_workspaces(token: str) -> List[Dict]:\n",
    "    \"\"\"Get all accessible workspaces\"\"\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    response = requests.get(f\"{FABRIC_API}/workspaces\", headers=headers, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    return response.json().get(\"value\", [])\n",
    "\n",
    "def get_workspace_items(workspace_id: str, token: str) -> List[Dict]:\n",
    "    \"\"\"Get all items in a workspace\"\"\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    response = requests.get(f\"{FABRIC_API}/workspaces/{workspace_id}/items\", headers=headers, timeout=60)\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get(\"value\", [])\n",
    "    return []\n",
    "\n",
    "def collect_capacity_metrics(capacities: List[Dict], workspaces: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Transform capacity data into metrics format\"\"\"\n",
    "    metrics = []\n",
    "    \n",
    "    for capacity in capacities:\n",
    "        capacity_id = capacity.get('id')\n",
    "        capacity_workspaces = [ws for ws in workspaces if ws.get('capacityId') == capacity_id]\n",
    "        \n",
    "        # Determine capacity type\n",
    "        sku = capacity.get('sku', '').upper()\n",
    "        if sku.startswith('F'):\n",
    "            capacity_type = \"Fabric (F-SKU)\"\n",
    "        elif sku.startswith('P') and not sku.startswith('PP'):\n",
    "            capacity_type = \"Premium (P-SKU)\"\n",
    "        elif 'PREMIUM PER USER' in capacity.get('displayName', '').upper():\n",
    "            capacity_type = \"Premium Per User (PPU)\"\n",
    "        else:\n",
    "            capacity_type = f\"Other ({sku})\"\n",
    "        \n",
    "        metric = {\n",
    "            \"TimeGenerated\": iso_now(),\n",
    "            \"CapacityId\": capacity_id,\n",
    "            \"CapacityName\": capacity.get('displayName', 'Unknown'),\n",
    "            \"CapacityType\": capacity_type,\n",
    "            \"CapacitySku\": sku,\n",
    "            \"Region\": capacity.get('region', 'Unknown'),\n",
    "            \"State\": capacity.get('state', 'Unknown'),\n",
    "            \"WorkspaceCount\": len(capacity_workspaces),\n",
    "            \"MonitoringMethod\": \"workspace-based\"\n",
    "        }\n",
    "        metrics.append(metric)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def collect_workspace_workloads(workspaces: List[Dict], token: str) -> List[Dict]:\n",
    "    \"\"\"Collect workload data from workspace items\"\"\"\n",
    "    workloads = []\n",
    "    \n",
    "    for workspace in workspaces:\n",
    "        workspace_id = workspace.get('id')\n",
    "        workspace_name = workspace.get('displayName', 'Unknown')\n",
    "        capacity_id = workspace.get('capacityId', '')\n",
    "        \n",
    "        items = get_workspace_items(workspace_id, token)\n",
    "        \n",
    "        for item in items:\n",
    "            workload = {\n",
    "                \"TimeGenerated\": iso_now(),\n",
    "                \"CapacityId\": capacity_id,\n",
    "                \"WorkspaceId\": workspace_id,\n",
    "                \"WorkspaceName\": workspace_name,\n",
    "                \"WorkloadType\": item.get('type', 'Unknown'),\n",
    "                \"ItemId\": item.get('id'),\n",
    "                \"ItemName\": item.get('displayName', 'Unknown'),\n",
    "                \"ItemDescription\": item.get('description', ''),\n",
    "                \"State\": \"Active\"  # Items exist = active\n",
    "            }\n",
    "            workloads.append(workload)\n",
    "    \n",
    "    return workloads\n",
    "\n",
    "def collect_workspace_events(workspaces: List[Dict], token: str) -> List[Dict]:\n",
    "    \"\"\"Collect workspace event data\"\"\"\n",
    "    events = []\n",
    "    \n",
    "    for workspace in workspaces:\n",
    "        workspace_id = workspace.get('id')\n",
    "        items = get_workspace_items(workspace_id, token)\n",
    "        \n",
    "        # Count items by type\n",
    "        item_types = {}\n",
    "        for item in items:\n",
    "            item_type = item.get('type', 'Unknown')\n",
    "            item_types[item_type] = item_types.get(item_type, 0) + 1\n",
    "        \n",
    "        event = {\n",
    "            \"TimeGenerated\": iso_now(),\n",
    "            \"EventType\": \"WorkspaceInventory\",\n",
    "            \"WorkspaceId\": workspace_id,\n",
    "            \"WorkspaceName\": workspace.get('displayName', 'Unknown'),\n",
    "            \"WorkspaceType\": workspace.get('type', 'Unknown'),\n",
    "            \"CapacityId\": workspace.get('capacityId', ''),\n",
    "            \"ItemCount\": len(items),\n",
    "            \"ItemTypes\": json.dumps(item_types),\n",
    "            \"IsOnDedicatedCapacity\": workspace.get('capacityId') is not None\n",
    "        }\n",
    "        events.append(event)\n",
    "    \n",
    "    return events\n",
    "\n",
    "def collect_item_details(workspaces: List[Dict], token: str) -> List[Dict]:\n",
    "    \"\"\"Collect detailed item information\"\"\"\n",
    "    details = []\n",
    "    \n",
    "    for workspace in workspaces:\n",
    "        workspace_id = workspace.get('id')\n",
    "        workspace_name = workspace.get('displayName', 'Unknown')\n",
    "        capacity_id = workspace.get('capacityId', '')\n",
    "        \n",
    "        items = get_workspace_items(workspace_id, token)\n",
    "        \n",
    "        for item in items:\n",
    "            detail = {\n",
    "                \"TimeGenerated\": iso_now(),\n",
    "                \"WorkspaceId\": workspace_id,\n",
    "                \"WorkspaceName\": workspace_name,\n",
    "                \"CapacityId\": capacity_id,\n",
    "                \"ItemId\": item.get('id'),\n",
    "                \"ItemName\": item.get('displayName', 'Unknown'),\n",
    "                \"ItemType\": item.get('type', 'Unknown'),\n",
    "                \"ItemDescription\": item.get('description', ''),\n",
    "                \"IsOnDedicatedCapacity\": capacity_id != ''\n",
    "            }\n",
    "            details.append(detail)\n",
    "    \n",
    "    return details\n",
    "\n",
    "print(\"âœ… Data collection functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47430153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication Setup\n",
    "print(\"ðŸ” Setting up authentication...\")\n",
    "\n",
    "# Get client secret\n",
    "if use_key_vault:\n",
    "    print(\"   ðŸ“‹ Getting secret from Key Vault...\")\n",
    "    client_secret = get_secret_from_key_vault(key_vault_uri, key_vault_secret_name)\n",
    "else:\n",
    "    print(\"   ðŸ“‹ Using environment variable...\")\n",
    "\n",
    "# Acquire tokens\n",
    "print(\"   ðŸŽ« Acquiring Fabric API token...\")\n",
    "fabric_token = acquire_token(tenant_id, client_id, client_secret, FABRIC_SCOPE)\n",
    "\n",
    "print(\"   ðŸŽ« Acquiring Monitor API token...\")\n",
    "monitor_token = acquire_token(tenant_id, client_id, client_secret, MONITOR_SCOPE)\n",
    "\n",
    "print(\"âœ… Authentication completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d32b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collection\n",
    "print(\"ðŸ“Š Starting data collection...\")\n",
    "\n",
    "# Get base data\n",
    "print(\"   ðŸ¢ Fetching capacities...\")\n",
    "all_capacities = get_capacities(fabric_token)\n",
    "\n",
    "print(\"   ðŸ“ Fetching workspaces...\")\n",
    "all_workspaces = get_workspaces(fabric_token)\n",
    "\n",
    "# Filter capacities if specific IDs provided\n",
    "if capacity_ids:\n",
    "    target_ids = [cid.lower() for cid in capacity_ids]\n",
    "    capacities_to_monitor = [cap for cap in all_capacities \n",
    "                           if cap.get('id', '').lower() in target_ids]\n",
    "    print(f\"   ðŸŽ¯ Filtered to {len(capacities_to_monitor)} specific capacities\")\n",
    "else:\n",
    "    capacities_to_monitor = all_capacities\n",
    "    print(f\"   ðŸŒ Monitoring all {len(capacities_to_monitor)} accessible capacities\")\n",
    "\n",
    "# Collect monitoring data\n",
    "print(\"   ðŸ“ˆ Collecting capacity metrics...\")\n",
    "capacity_metrics = collect_capacity_metrics(capacities_to_monitor, all_workspaces)\n",
    "\n",
    "print(\"   ðŸ’¼ Collecting workload data...\")\n",
    "workload_data = collect_workspace_workloads(all_workspaces, fabric_token)\n",
    "\n",
    "print(\"   ðŸ“‹ Collecting workspace events...\")\n",
    "workspace_events = collect_workspace_events(all_workspaces, fabric_token)\n",
    "\n",
    "print(\"   ðŸ“¦ Collecting item details...\")\n",
    "item_details = collect_item_details(all_workspaces, fabric_token)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nðŸ“Š Collection Summary:\")\n",
    "print(f\"   Capacity metrics: {len(capacity_metrics)} records\")\n",
    "print(f\"   Workload data: {len(workload_data)} records\")\n",
    "print(f\"   Workspace events: {len(workspace_events)} records\")\n",
    "print(f\"   Item details: {len(item_details)} records\")\n",
    "\n",
    "print(\"âœ… Data collection completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ebaafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send Data to Log Analytics\n",
    "print(\"ðŸ“¤ Sending data to Azure Log Analytics...\")\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Send capacity metrics\n",
    "if capacity_metrics:\n",
    "    print(f\"   ðŸ“ˆ Sending {len(capacity_metrics)} capacity metrics...\")\n",
    "    result = post_to_log_analytics(dcr_endpoint_host, dcr_immutable_id, \n",
    "                                 stream_capacity_metrics, capacity_metrics, monitor_token)\n",
    "    results[\"capacity_metrics\"] = result\n",
    "    print(f\"      âœ… Sent {result['sent']} records in {result['batches']} batches\")\n",
    "\n",
    "# Send workload data\n",
    "if workload_data:\n",
    "    print(f\"   ðŸ’¼ Sending {len(workload_data)} workload records...\")\n",
    "    result = post_to_log_analytics(dcr_endpoint_host, dcr_immutable_id, \n",
    "                                 stream_capacity_workloads, workload_data, monitor_token)\n",
    "    results[\"workload_data\"] = result\n",
    "    print(f\"      âœ… Sent {result['sent']} records in {result['batches']} batches\")\n",
    "\n",
    "# Send workspace events (if DCR stream exists)\n",
    "if workspace_events:\n",
    "    try:\n",
    "        print(f\"   ðŸ“‹ Sending {len(workspace_events)} workspace events...\")\n",
    "        result = post_to_log_analytics(dcr_endpoint_host, dcr_immutable_id, \n",
    "                                     stream_workspace_events, workspace_events, monitor_token)\n",
    "        results[\"workspace_events\"] = result\n",
    "        print(f\"      âœ… Sent {result['sent']} records in {result['batches']} batches\")\n",
    "    except RuntimeError as e:\n",
    "        if \"not configured\" in str(e):\n",
    "            print(f\"      âš ï¸  Stream {stream_workspace_events} not configured in DCR - skipping\")\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "# Send item details (if DCR stream exists)\n",
    "if item_details:\n",
    "    try:\n",
    "        print(f\"   ðŸ“¦ Sending {len(item_details)} item details...\")\n",
    "        result = post_to_log_analytics(dcr_endpoint_host, dcr_immutable_id, \n",
    "                                     stream_item_details, item_details, monitor_token)\n",
    "        results[\"item_details\"] = result\n",
    "        print(f\"      âœ… Sent {result['sent']} records in {result['batches']} batches\")\n",
    "    except RuntimeError as e:\n",
    "        if \"not configured\" in str(e):\n",
    "            print(f\"      âš ï¸  Stream {stream_item_details} not configured in DCR - skipping\")\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "print(\"\\nâœ… Data ingestion completed successfully!\")\n",
    "print(f\"\\nðŸ“‹ Final Results:\")\n",
    "for stream_type, result in results.items():\n",
    "    print(f\"   {stream_type}: {result['sent']} records sent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501308f6",
   "metadata": {},
   "source": [
    "## âœ… Monitoring Complete\n",
    "\n",
    "### Infrastructure Setup\n",
    "**Important**: Before running this notebook, deploy the required Azure infrastructure using:\n",
    "- **Terraform**: `logAnalytics/terraform/` folder\n",
    "- **Bicep**: `logAnalytics/bicep/` folder\n",
    "\n",
    "Both create:\n",
    "- Log Analytics Workspace with custom tables\n",
    "- Data Collection Endpoint (DCE)\n",
    "- Data Collection Rules (DCR) with all required streams\n",
    "\n",
    "### Data Collected\n",
    "- **Capacity Metrics**: Basic capacity inventory with workspace counts\n",
    "- **Workload Data**: Item-based workload tracking across workspaces  \n",
    "- **Workspace Events**: Workspace inventory and item distribution\n",
    "- **Item Details**: Enhanced metadata for all Fabric items\n",
    "\n",
    "### Next Steps\n",
    "1. **Deploy Infrastructure**: Use Terraform or Bicep templates first\n",
    "2. **Configure Environment**: Set DCR_ENDPOINT_HOST and DCR_IMMUTABLE_ID from deployment outputs\n",
    "3. **Schedule this notebook** to run regularly (daily/weekly)\n",
    "4. **Build dashboards** in Azure Monitor/Power BI using the collected data\n",
    "5. **Set up alerts** based on capacity utilization trends\n",
    "\n",
    "### KQL Query Examples\n",
    "```kql\n",
    "// Capacity utilization overview\n",
    "FabricCapacityMetrics_CL\n",
    "| summarize arg_max(TimeGenerated, *) by CapacityId\n",
    "| project CapacityName, CapacityType, WorkspaceCount, Region\n",
    "\n",
    "// Item distribution by type\n",
    "FabricCapacityWorkloads_CL\n",
    "| summarize count() by WorkloadType, CapacityId\n",
    "| order by count_ desc\n",
    "\n",
    "// Workspace activity\n",
    "FabricWorkspaceEvents_CL\n",
    "| where TimeGenerated > ago(7d)\n",
    "| summarize TotalItems = sum(ItemCount) by WorkspaceName\n",
    "```\n",
    "\n",
    "### Infrastructure Files\n",
    "- `logAnalytics/terraform/` - Terraform templates for Azure resources\n",
    "- `logAnalytics/bicep/` - Bicep templates for Azure resources\n",
    "- `logAnalytics/common/` - Shared JSON templates for table and DCR definitions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
