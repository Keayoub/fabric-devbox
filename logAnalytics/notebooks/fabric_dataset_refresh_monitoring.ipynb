{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "632048f8",
   "metadata": {},
   "source": [
    "# Fabric Dataset Refresh Monitoring\n",
    "\n",
    "Collects **dataset refresh operations** and **metadata** from Fabric REST APIs and sends to Azure Log Analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f2f3c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# === One-time installs per session Or Use Fabric Environment ===\n",
    "%pip install --quiet msal requests azure-identity azure-keyvault-secrets python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2f4dcce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment variables loaded\n",
      "Workspace mode: Specific workspaces\n",
      "Dataset mode: All datasets\n",
      "Lookback: 42000 hours\n"
     ]
    }
   ],
   "source": [
    "# === Parameters (mark this as a parameter cell in Fabric) ===\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Multi-workspace monitoring for security and compliance\n",
    "# Empty list = monitor ALL accessible workspaces (recommended for security)\n",
    "# Specific workspaces = [\"workspace-id-1\", \"workspace-id-2\"]\n",
    "workspace_ids = (\n",
    "    [os.getenv(\"FABRIC_WORKSPACE_ID\")] if os.getenv(\"FABRIC_WORKSPACE_ID\") else []\n",
    ")\n",
    "\n",
    "# Dataset filtering - leave empty to monitor all datasets in workspaces\n",
    "dataset_ids = []  # Add specific dataset IDs here, or leave empty for all datasets\n",
    "# Example: dataset_ids = [\"8c4003da-3254-4a1a-b07e-ac62cb86b5cf\", \"94a8ba89-ac1b-4a6c-b310-775f3967f29f\"]\n",
    "\n",
    "lookback_hours = 42000  # Number of hours to look back for refresh history\n",
    "\n",
    "dcr_endpoint_host = os.getenv(\"DCR_ENDPOINT_HOST\")\n",
    "dcr_immutable_id = os.getenv(\"DCR_IMMUTABLE_ID\")\n",
    "stream_dataset_refresh = \"Custom-FabricDatasetRefresh_CL\"\n",
    "stream_dataset_metadata = \"Custom-FabricDatasetMetadata_CL\"\n",
    "\n",
    "tenant_id = os.getenv(\"FABRIC_TENANT_ID\")\n",
    "client_id = os.getenv(\"FABRIC_APP_ID\")\n",
    "client_secret_env = os.getenv(\"FABRIC_APP_SECRET\")\n",
    "\n",
    "use_key_vault = False\n",
    "use_managed_identity = False\n",
    "key_vault_uri = os.getenv(\n",
    "    \"AZURE_KEY_VAULT_URI\", \"https://kaydemokeyvault.vault.azure.net/\"\n",
    ")\n",
    "key_vault_secret_name = os.getenv(\n",
    "    \"AZURE_KEY_VAULT_SECRET_NAME\", \"FabricServicePrincipal\"\n",
    ")\n",
    "\n",
    "if not all([tenant_id, client_id, dcr_endpoint_host, dcr_immutable_id]):\n",
    "    missing = []\n",
    "    if not tenant_id:\n",
    "        missing.append(\"FABRIC_TENANT_ID\")\n",
    "    if not client_id:\n",
    "        missing.append(\"FABRIC_APP_ID\")\n",
    "    if not dcr_endpoint_host:\n",
    "        missing.append(\"DCR_ENDPOINT_HOST\")\n",
    "    if not dcr_immutable_id:\n",
    "        missing.append(\"DCR_IMMUTABLE_ID\")\n",
    "    print(f\"âŒ Missing: {', '.join(missing)}\")\n",
    "else:\n",
    "    print(\"âœ… Environment variables loaded\")\n",
    "\n",
    "print(f\"Workspace mode: {'Specific workspaces' if workspace_ids else 'Auto-discovery'}\")\n",
    "print(f\"Dataset mode: {'Specific datasets' if dataset_ids else 'All datasets'}\")\n",
    "print(f\"Lookback: {lookback_hours} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e02a7910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Functions loaded\n"
     ]
    }
   ],
   "source": [
    "# === Define main functions ===\n",
    "import os, json, time, datetime as dt\n",
    "import requests\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def get_secret_from_kv(vault_uri: str, secret_name: str, tenant_id: str = None, client_id: str = None, client_secret: str = None, use_managed_identity: bool = False) -> str:\n",
    "    try:\n",
    "        from azure.keyvault.secrets import SecretClient\n",
    "        if use_managed_identity:\n",
    "            from azure.identity import ManagedIdentityCredential\n",
    "            credential = ManagedIdentityCredential()\n",
    "        else:\n",
    "            from azure.identity import ClientSecretCredential\n",
    "            credential = ClientSecretCredential(tenant_id=tenant_id, client_id=client_id, client_secret=client_secret)\n",
    "        client = SecretClient(vault_url=vault_uri, credential=credential)\n",
    "        return client.get_secret(secret_name).value\n",
    "    except Exception as e:\n",
    "        print(f\"[KeyVault] Failed: {e}\")\n",
    "        return None\n",
    "\n",
    "FABRIC_SCOPE = \"https://api.fabric.microsoft.com/.default\"\n",
    "MONITOR_SCOPE = \"https://monitor.azure.com/.default\"\n",
    "FABRIC_API = \"https://api.fabric.microsoft.com/v1\"\n",
    "\n",
    "def acquire_token_client_credentials(tenant: str, client_id: str, client_secret: str, scope: str) -> str:\n",
    "    import msal\n",
    "    authority = f\"https://login.microsoftonline.com/{tenant}\"\n",
    "    app = msal.ConfidentialClientApplication(client_id, authority=authority, client_credential=client_secret)\n",
    "    result = app.acquire_token_for_client(scopes=[scope])\n",
    "    if \"access_token\" not in result:\n",
    "        raise RuntimeError(f\"Failed to get token for {scope}: {result}\")\n",
    "    token = result[\"access_token\"]\n",
    "    print(f\"âœ… Token acquired for {scope}\")\n",
    "    return token\n",
    "\n",
    "def iso_now() -> str:\n",
    "    return dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc).isoformat().replace(\"+00:00\", \"Z\")\n",
    "\n",
    "def parse_iso(s: str) -> dt.datetime:\n",
    "    if not s:\n",
    "        return None\n",
    "    if s.endswith(\"Z\"):\n",
    "        s = s[:-1] + \"+00:00\"\n",
    "    parsed = dt.datetime.fromisoformat(s)\n",
    "    if parsed.tzinfo is None:\n",
    "        parsed = parsed.replace(tzinfo=dt.timezone.utc)\n",
    "    return parsed\n",
    "\n",
    "def list_workspace_datasets(workspace_id: str, token: str) -> List[Dict[str, Any]]:\n",
    "    url = f\"{FABRIC_API}/workspaces/{workspace_id}/datasets\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    try:\n",
    "        r = requests.get(url, headers=headers, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        return data.get(\"value\", [])\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to get datasets for workspace {workspace_id}: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_dataset_refresh_history(dataset_id: str, token: str, top: int = 200) -> List[Dict[str, Any]]:\n",
    "    url = f\"{FABRIC_API}/datasets/{dataset_id}/refreshes?$top={top}\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    try:\n",
    "        r = requests.get(url, headers=headers, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        return data.get(\"value\", [])\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to get refresh history for dataset {dataset_id}: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_dataset_metadata(dataset_id: str, token: str) -> Dict[str, Any]:\n",
    "    url = f\"{FABRIC_API}/datasets/{dataset_id}\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    try:\n",
    "        r = requests.get(url, headers=headers, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to get metadata for dataset {dataset_id}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def map_dataset_refresh(workspace_id: str, dataset_id: str, dataset_name: str, refresh: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    start_time = refresh.get(\"startTime\")\n",
    "    end_time = refresh.get(\"endTime\")\n",
    "    \n",
    "    duration_ms = None\n",
    "    if start_time and end_time:\n",
    "        try:\n",
    "            start_dt = parse_iso(start_time)\n",
    "            end_dt = parse_iso(end_time)\n",
    "            if start_dt and end_dt:\n",
    "                duration_ms = int((end_dt - start_dt).total_seconds() * 1000)\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    return {\n",
    "        \"TimeGenerated\": end_time or start_time or iso_now(),\n",
    "        \"WorkspaceId\": workspace_id,\n",
    "        \"DatasetId\": dataset_id,\n",
    "        \"DatasetName\": dataset_name,\n",
    "        \"RefreshId\": refresh.get(\"id\"),\n",
    "        \"RefreshType\": refresh.get(\"refreshType\"),\n",
    "        \"Status\": refresh.get(\"status\"),\n",
    "        \"StartTime\": start_time,\n",
    "        \"EndTime\": end_time,\n",
    "        \"DurationMs\": duration_ms,\n",
    "        \"ServicePrincipalId\": refresh.get(\"servicePrincipalId\"),\n",
    "        \"ErrorCode\": refresh.get(\"errorCode\"),\n",
    "        \"ErrorMessage\": refresh.get(\"errorMessage\"),\n",
    "        \"RequestId\": refresh.get(\"requestId\"),\n",
    "    }\n",
    "\n",
    "def map_dataset_metadata(workspace_id: str, dataset: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"TimeGenerated\": iso_now(),\n",
    "        \"WorkspaceId\": workspace_id,\n",
    "        \"DatasetId\": dataset.get(\"id\"),\n",
    "        \"DatasetName\": dataset.get(\"name\"),\n",
    "        \"Description\": dataset.get(\"description\"),\n",
    "        \"ConfiguredBy\": dataset.get(\"configuredBy\"),\n",
    "        \"CreatedDate\": dataset.get(\"createdDate\"),\n",
    "        \"ModifiedDate\": dataset.get(\"modifiedDate\"),\n",
    "        \"ContentProviderType\": dataset.get(\"contentProviderType\"),\n",
    "        \"DatasourceType\": dataset.get(\"datasourceType\"),\n",
    "        \"IsOnPremGatewayRequired\": dataset.get(\"isOnPremGatewayRequired\", False),\n",
    "        \"IsRefreshable\": dataset.get(\"isRefreshable\", False),\n",
    "        \"AddRowsAPIEnabled\": dataset.get(\"addRowsAPIEnabled\", False),\n",
    "    }\n",
    "\n",
    "def list_accessible_workspaces(token: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Get all workspaces accessible to the service principal\"\"\"\n",
    "    url = f\"{FABRIC_API}/workspaces\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    try:\n",
    "        r = requests.get(url, headers=headers, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        return data.get(\"value\", [])\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to list workspaces: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_workspace_details(workspace_id: str, token: str) -> Dict[str, Any]:\n",
    "    \"\"\"Get detailed workspace information\"\"\"\n",
    "    url = f\"{FABRIC_API}/workspaces/{workspace_id}\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    try:\n",
    "        r = requests.get(url, headers=headers, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to get workspace details for {workspace_id}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def list_workspace_items(workspace_id: str, token: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Get all items in a workspace (alternative to datasets endpoint)\"\"\"\n",
    "    url = f\"{FABRIC_API}/workspaces/{workspace_id}/items\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    try:\n",
    "        r = requests.get(url, headers=headers, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        items = data.get(\"value\", [])\n",
    "        # Only get Dataset and SemanticModel types, and convert to dataset-like format\n",
    "        datasets = []\n",
    "        for item in items:\n",
    "            if item.get(\"type\") in [\"Dataset\", \"SemanticModel\"]:\n",
    "                # Convert item format to dataset format for compatibility\n",
    "                dataset = {\n",
    "                    \"id\": item.get(\"id\"),\n",
    "                    \"name\": item.get(\"displayName\", item.get(\"name\", \"Unknown\")),\n",
    "                    \"type\": item.get(\"type\"),\n",
    "                    \"description\": item.get(\"description\"),\n",
    "                    # Note: Items API doesn't provide refresh metadata\n",
    "                }\n",
    "                datasets.append(dataset)\n",
    "        return datasets\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to get items for workspace {workspace_id}: {e}\")\n",
    "        return []\n",
    "\n",
    "def post_rows_to_dcr(endpoint_host: str, dcr_id: str, stream_name: str, rows: List[Dict[str, Any]], monitor_token: str):\n",
    "    if not rows:\n",
    "        return {\"sent\": 0, \"batches\": 0}\n",
    "    \n",
    "    MAX_BYTES = 950_000\n",
    "    batch, batches, sent, size = [], 0, 0, 2\n",
    "    \n",
    "    def flush():\n",
    "        nonlocal batches, sent, batch, size\n",
    "        if not batch:\n",
    "            return\n",
    "        url = f\"https://{endpoint_host}/dataCollectionRules/{dcr_id}/streams/{stream_name}?api-version=2023-01-01\"\n",
    "        headers = {\"Authorization\": f\"Bearer {monitor_token}\", \"Content-Type\": \"application/json\"}\n",
    "        resp = requests.post(url, headers=headers, data=json.dumps(batch), timeout=60)\n",
    "        if resp.status_code >= 400:\n",
    "            raise RuntimeError(f\"Ingestion failed ({resp.status_code}): {resp.text[:500]}\")\n",
    "        batches += 1\n",
    "        sent += len(batch)\n",
    "        batch, size = [], 2\n",
    "    \n",
    "    for row in rows:\n",
    "        s = len(json.dumps(row, separators=(\",\", \":\")))\n",
    "        if size + s + (1 if batch else 0) > MAX_BYTES:\n",
    "            flush()\n",
    "        batch.append(row)\n",
    "        size += s + (1 if batch else 0)\n",
    "    flush()\n",
    "    return {\"sent\": sent, \"batches\": batches}\n",
    "\n",
    "print(\"âœ… Functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99d11d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using environment variable\n",
      "âœ… Client secret resolved\n",
      "âœ… Token acquired for https://api.fabric.microsoft.com/.default\n",
      "âœ… Token acquired for https://monitor.azure.com/.default\n",
      "ðŸ” Checking accessible workspaces...\n",
      "Found 1 accessible workspaces:\n",
      "  - Unknown (8457f746-f2d9-4d27-8221-5714601e40c6)\n",
      "\n",
      "ðŸ” Checking specified workspace IDs:\n",
      "  âœ… 8457f746-f2d9-4d27-8221-5714601e40c6 - None\n",
      "\n",
      "Collecting dataset refresh data from last 42000 hours...\n",
      "\n",
      "Processing workspace: 8457f746-f2d9-4d27-8221-5714601e40c6\n",
      "  Workspace name: WWI_Samples\n",
      "  Workspace type: Workspace\n",
      "âŒ Failed to get datasets for workspace 8457f746-f2d9-4d27-8221-5714601e40c6: 404 Client Error: Not Found for url: https://api.fabric.microsoft.com/v1/workspaces/8457f746-f2d9-4d27-8221-5714601e40c6/datasets\n",
      "  Trying alternative items endpoint...\n",
      "  Found 14 datasets via items endpoint\n",
      "  Processing dataset: LH_WWI (292521d7-2a1d-475a-b534-ea66d422d925)\n",
      "    Skipping refresh history (using items API)\n",
      "  Processing dataset: DataflowsStagingLakehouse (64faff34-f495-4011-b506-21d423884707)\n",
      "    Skipping refresh history (using items API)\n",
      "  Processing dataset: DataflowsStagingWarehouse (4d24b32e-704d-4262-8830-c79056bd6ee0)\n",
      "    Skipping refresh history (using items API)\n",
      "  Processing dataset: WH_WWI (1c5dac3a-e2ac-4be6-95e6-dfdcd8b9b88e)\n",
      "    Skipping refresh history (using items API)\n",
      "  Processing dataset: StagingLakehouseForDataflows_20250618183751 (fe606de1-f853-40bc-a31d-f47a39668654)\n",
      "    Skipping refresh history (using items API)\n",
      "  Processing dataset: StagingWarehouseForDataflows_20250618183815 (149a2d7b-4adb-401a-a7ea-d0cf6b16527a)\n",
      "    Skipping refresh history (using items API)\n",
      "  Processing dataset: LH_Silver (38d7399e-bd7c-444e-9cc8-9ef09c23d6a7)\n",
      "    Skipping refresh history (using items API)\n",
      "  Processing dataset: DataMartWWI (065e085e-b973-4a03-9277-be5478335150)\n",
      "    Skipping refresh history (using items API)\n",
      "  Processing dataset: ProcutsStock (94a8ba89-ac1b-4a6c-b310-775f3967f29f)\n",
      "    Skipping refresh history (using items API)\n",
      "  Processing dataset: WWI_Importers (465998c6-f457-43ea-817c-8687483e8608)\n",
      "    Skipping refresh history (using items API)\n",
      "  Processing dataset: Weather_Analysis (2dc9672c-34f7-4bf8-9591-850ac3a67127)\n",
      "    Skipping refresh history (using items API)\n",
      "  Processing dataset: SampleDB (01999642-1338-41a5-8451-342e48761e02)\n",
      "    Skipping refresh history (using items API)\n",
      "  Processing dataset: MLV_Sample (da06de12-fd75-4954-a48f-4e9f21388e59)\n",
      "    Skipping refresh history (using items API)\n",
      "  Processing dataset: sample_CM (8c4003da-3254-4a1a-b07e-ac62cb86b5cf)\n",
      "    Skipping refresh history (using items API)\n",
      "Collection Summary:\n",
      "Total Datasets: 14\n",
      "Total Refreshes: 0\n",
      "Metadata Records: 14\n",
      "Refresh Records: 0\n",
      "Sending dataset metadata...\n",
      "âœ… Done!\n",
      "{\n",
      "  \"dataset_metadata\": {\n",
      "    \"sent\": 14,\n",
      "    \"batches\": 1\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# === Main ===\n",
    "client_secret = None\n",
    "\n",
    "if client_secret_env:\n",
    "    client_secret = client_secret_env\n",
    "    print(\"âœ… Using environment variable\")\n",
    "elif use_key_vault:\n",
    "    if use_managed_identity:\n",
    "        client_secret = get_secret_from_kv(key_vault_uri, key_vault_secret_name, use_managed_identity=True)\n",
    "    else:\n",
    "        temp_secret = os.getenv(\"FABRIC_APP_SECRET\")\n",
    "        if not temp_secret:\n",
    "            raise RuntimeError(\"FABRIC_APP_SECRET required for Key Vault access\")\n",
    "        client_secret = get_secret_from_kv(key_vault_uri, key_vault_secret_name, tenant_id, client_id, temp_secret, use_managed_identity=False)\n",
    "\n",
    "if not client_secret:\n",
    "    raise RuntimeError(\"Client secret not found\")\n",
    "\n",
    "print(\"âœ… Client secret resolved\")\n",
    "\n",
    "fabric_token = acquire_token_client_credentials(tenant_id, client_id, client_secret, FABRIC_SCOPE)\n",
    "monitor_token = acquire_token_client_credentials(tenant_id, client_id, client_secret, MONITOR_SCOPE)\n",
    "\n",
    "# First, let's check what workspaces are accessible\n",
    "print(\"ðŸ” Checking accessible workspaces...\")\n",
    "accessible_workspaces = list_accessible_workspaces(fabric_token)\n",
    "print(f\"Found {len(accessible_workspaces)} accessible workspaces:\")\n",
    "\n",
    "for ws in accessible_workspaces[:10]:  # Show first 10\n",
    "    print(f\"  - {ws.get('name', 'Unknown')} ({ws.get('id', 'No ID')})\")\n",
    "\n",
    "if len(accessible_workspaces) > 10:\n",
    "    print(f\"  ... and {len(accessible_workspaces) - 10} more\")\n",
    "\n",
    "# Check if specified workspace IDs are accessible\n",
    "if workspace_ids:\n",
    "    accessible_ids = {ws.get('id') for ws in accessible_workspaces}\n",
    "    print(f\"\\nðŸ” Checking specified workspace IDs:\")\n",
    "    for ws_id in workspace_ids:\n",
    "        if ws_id in accessible_ids:\n",
    "            ws_name = next((ws.get('name') for ws in accessible_workspaces if ws.get('id') == ws_id), 'Unknown')\n",
    "            print(f\"  âœ… {ws_id} - {ws_name}\")\n",
    "        else:\n",
    "            print(f\"  âŒ {ws_id} - NOT ACCESSIBLE\")\n",
    "\n",
    "now = dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc)\n",
    "cutoff_time = now - dt.timedelta(hours=lookback_hours)\n",
    "\n",
    "print(f\"\\nCollecting dataset refresh data from last {lookback_hours} hours...\")\n",
    "\n",
    "refresh_rows = []\n",
    "metadata_rows = []\n",
    "total_datasets = 0\n",
    "total_refreshes = 0\n",
    "\n",
    "# Use accessible workspaces if no specific ones provided\n",
    "workspaces_to_process = workspace_ids if workspace_ids else [ws.get('id') for ws in accessible_workspaces]\n",
    "\n",
    "for workspace_id in workspaces_to_process:\n",
    "    print(f\"\\nProcessing workspace: {workspace_id}\")\n",
    "    \n",
    "    # Get workspace details\n",
    "    workspace_details = get_workspace_details(workspace_id, fabric_token)\n",
    "    if workspace_details:\n",
    "        print(f\"  Workspace name: {workspace_details.get('displayName', 'Unknown')}\")\n",
    "        print(f\"  Workspace type: {workspace_details.get('type', 'Unknown')}\")\n",
    "    \n",
    "    # Try the datasets endpoint first\n",
    "    datasets = list_workspace_datasets(workspace_id, fabric_token)\n",
    "    using_items_api = False\n",
    "    \n",
    "    # If datasets endpoint fails, try items endpoint as fallback\n",
    "    if not datasets:\n",
    "        print(\"  Trying alternative items endpoint...\")\n",
    "        datasets = list_workspace_items(workspace_id, fabric_token)\n",
    "        using_items_api = True\n",
    "        if datasets:\n",
    "            print(f\"  Found {len(datasets)} datasets via items endpoint\")\n",
    "        else:\n",
    "            print(\"  No datasets found via either endpoint\")\n",
    "            continue\n",
    "    else:\n",
    "        print(f\"  Found {len(datasets)} datasets via datasets endpoint\")\n",
    "    \n",
    "    # Process each dataset once\n",
    "    for dataset in datasets:\n",
    "        dataset_id = dataset.get(\"id\")\n",
    "        dataset_name = dataset.get(\"name\", dataset.get(\"displayName\", \"Unknown\"))\n",
    "        \n",
    "        if dataset_ids and dataset_id not in dataset_ids:\n",
    "            continue\n",
    "        \n",
    "        print(f\"  Processing dataset: {dataset_name} ({dataset_id})\")\n",
    "        total_datasets += 1\n",
    "        \n",
    "        # Skip refresh history collection if using items API since those IDs might not work with dataset APIs\n",
    "        if using_items_api:\n",
    "            print(f\"    Skipping refresh history (using items API)\")\n",
    "            # Create a basic metadata record from items data\n",
    "            metadata_row = map_dataset_metadata(workspace_id, dataset)\n",
    "            metadata_rows.append(metadata_row)\n",
    "        else:\n",
    "            # Normal processing for datasets API\n",
    "            detailed_metadata = get_dataset_metadata(dataset_id, fabric_token)\n",
    "            if detailed_metadata:\n",
    "                metadata_row = map_dataset_metadata(workspace_id, {**dataset, **detailed_metadata})\n",
    "                metadata_rows.append(metadata_row)\n",
    "            \n",
    "            refreshes = get_dataset_refresh_history(dataset_id, fabric_token)\n",
    "            \n",
    "            recent_refreshes = []\n",
    "            for refresh in refreshes:\n",
    "                refresh_time = parse_iso(refresh.get(\"endTime\") or refresh.get(\"startTime\"))\n",
    "                if refresh_time and refresh_time >= cutoff_time:\n",
    "                    recent_refreshes.append(refresh)\n",
    "            \n",
    "            print(f\"    Found {len(recent_refreshes)} recent refreshes\")\n",
    "            total_refreshes += len(recent_refreshes)\n",
    "            \n",
    "            for refresh in recent_refreshes:\n",
    "                refresh_row = map_dataset_refresh(workspace_id, dataset_id, dataset_name, refresh)\n",
    "                refresh_rows.append(refresh_row)\n",
    "\n",
    "print(f\"Collection Summary:\")\n",
    "print(f\"Total Datasets: {total_datasets}\")\n",
    "print(f\"Total Refreshes: {total_refreshes}\")\n",
    "print(f\"Metadata Records: {len(metadata_rows)}\")\n",
    "print(f\"Refresh Records: {len(refresh_rows)}\")\n",
    "\n",
    "summary = {}\n",
    "\n",
    "if metadata_rows:\n",
    "    print(\"Sending dataset metadata...\")\n",
    "    result = post_rows_to_dcr(dcr_endpoint_host, dcr_immutable_id, stream_dataset_metadata, metadata_rows, monitor_token)\n",
    "    summary[\"dataset_metadata\"] = result\n",
    "\n",
    "if refresh_rows:\n",
    "    print(\"Sending refresh history...\")\n",
    "    result = post_rows_to_dcr(dcr_endpoint_host, dcr_immutable_id, stream_dataset_refresh, refresh_rows, monitor_token)\n",
    "    summary[\"dataset_refreshes\"] = result\n",
    "\n",
    "print(\"âœ… Done!\")\n",
    "print(json.dumps(summary, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
